\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{titling}
\usepackage{amsmath,graphicx}
\usepackage{bm}
\usepackage{enumerate}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage[font=footnotesize]{caption}
\usepackage{subcaption}
\setlength{\droptitle}{-15mm}

\newcommand{\m}[1]{\mathbf{\bm{#1}}}
\newcommand{\R}{I\hspace{-4.4pt}R}

\title{Stat 666 Mini-Project \#2}
\date{}
\author{Mickey Warner, Phil White}

\begin{document}
\maketitle
\vspace{-20mm}
\section*{Introduction}
\setstretch{1.5}
\vspace{-4mm}
The purpose of this project is to analyze and compare data from randomly sampled olives from two adjacent agricultural regions of interest (regions 2 and 4). Measurements were taken of eight different fatty acids common to olives (palmitic, palmitoleic, stearic, oleic, linoleic, eicosanoic, linolenic, and eicosenoic); however, fatty acid measurements were missing for many olives in both regions. For both regions, we use the expectation-maximization (EM) algorithm to estimate the missing fatty acid measurements, we test whether the samples from region 2 and region 4 for the given year differ from their historical means, and we analyze whether the missing measurements are missing at random or whether there is an underlying pattern for the missing data.  Then, we test if the olives from the two regions have the same profile in terms of the eight fatty acids and whether linoleic and linolenic acids contribute any significant information about how the regions differ beyond the information already available in the other 6 acids.
\vspace{-4mm}

\section*{Handling missing values, EM algorithm}
\vspace{-4mm}
Many olives were missing one or more acid measurements, which prohibits us from performing analyses without first addressing this problem. One solution is to simply remove the olives from the dataset that do not have acid measurements. However, this would take away valuable information; therefore, we desire an approach that preserves as much information as possible. The expectation-maximization (EM) algorithm is a method for imputing missing values by predicting the missing acid measurements using the values of the acids that were measured, allowing us to keep all measurements. The EM algorithm performs best when variables are strongly correlated. Furthermore, the algorithm assumes the missing values are \emph{missing at random}, which we discuss in a later section.  Subsequent analyses in this report use data after running the EM algorithm.

We calculate the mean vector $\overline{\m{x}} = \frac{1}{n} \sum^{n}_{i=1} \m{x}_{i}$, where $\m{x}_i$ is the $p$-vector of measurements on the $i^{th}$ olive, and sample covariance matrix
\begin{equation*}
\m{S} = \frac{1}{n-1} \sum^{n}_{i=1} (\m{x}_{i} - \overline{\m{x}})(\m{x}_{i} - \overline{\m{x}})'
\end{equation*}
which is a measure of how correlated all the variables are with each other.  Specifically, the component in the $i^{\textrm{th}}$ row and the $j^{\textrm{th}}$ column of $\m{S}$ is the covariance between the $i^{\textrm{th}}$ and $j^{\textrm{th}}$ variables. At the start of the algorithm, we calculate $\overline{\m{x}}$ and $\m{S}$ using only known measurements.

Since we only use known values, it is possible that $\m{S}$ is not positive definite, which precludes us from carrying out the EM algorithm. We remedy this by forcing any nonpositive eigenvalues of $\m{S}$ to be slightly larger than 0, a procedure which changes $\m{S}$ minimally. Let the superscripts $(q)$ and $(p)$ denote partitions of $\m{x}_i$, $\overline{\m{x}}$, and $\m{S}$ corresponding to the missing components and the known values, respectively. Then, we carry out the following steps:
\begin{enumerate}
\item For each $\m{x}_i$ with missing values, use $\m{x}_i^{(p)}$ to predict $\m{x}_i^{(q)}$ by
\vspace{-3mm}
\begin{equation*}
\m{x}_i^{(q)} = \overline{\m{x}}^{(q)} + \m{S}^{(q)(p)}(\m{S}^{(p)(p)})^{-1}\left(\m{x}_i^{(p)}-\overline{\m{x}}^{(p)}\right). 
\end{equation*}
\vspace{-13mm}
\item Update $\overline{\m{x}}$ and $\m{S}$ using the new values in each $\m{x}_i$.
\item Repeat steps 1 and 2 until the predictions converge.
\end{enumerate}
\vspace{-4mm}

\section*{Tests on historical means}
\vspace{-4mm}
For the olives in regions two and four we want to test whether the means of the acid measurements differ significantly from their historical averages. We test 
\vspace{-3mm}
\begin{eqnarray*}
H_{02}:\m{\mu}_{02}=(1300,~120,~265,~7310,~820,~45,~65,~28)' \\
H_{04}:\m{\mu}_{04}=(1230,~105,~275,~7360,~830,~41,~75,~38)'
\end{eqnarray*}
where $H_{02}$ and $H_{04}$ are the null hypotheses for tests and regions two and four, respectively.

We use Hotelling's $T^2$ test statistic (a multivariate analogue of the univariate $t$ test statistic) defined as
\[ T^2 = n(\overline{\m{x}}-\m{\mu}_0)'\m{S}^{-1}(\overline{\m{x}}-\m{\mu}_0) \]
to test for significant departures from the hypothesized mean $\m{\mu}_0$. Large differences in the mean of any variable from the historical mean result in large $T^2$ statistics. We can convert $T^2$ into an $F$ distribution via
\[ \frac{\nu -p+1}{\nu p}T^2 = F \sim F_{p,\nu-p+1}. \]
where $\nu=n-1$ degrees of freedom and $p=8$ acids.

Table \ref{history} shows the test statistics and $p$-values for each region's null hypothesis. We fail to reject both hypotheses because there is insufficient evidence in either region to suggest that the means of the acids differ significantly from their historical means.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c | rrr}
Region & \multicolumn{1}{l}{$T^2$}  & \multicolumn{1}{l}{$F$}    & \multicolumn{1}{l}{$p$-value} \\ \hline \hline
     2 & 10.119 & 1.103  & 0.377     \\
     4 & 20.183 & 2.018  & 0.081     \\
\end{tabular}
\caption{Historical means comparison test results.}
\label{history}
\end{center}
\end{table}
\vspace{-10mm}

\section*{Analysis of missing data}
\vspace{-4mm}
As previously mentioned, the EM algorithm assumes that missing values are missing at random. That is, the measurements are not missing due to the values of other variables. However, it may be the case that measurements in one acid increase the likelihood of another variable to be missing, which invalidate the assumptions of the EM algorithm.

To analyze the missing data, let $\m{X}$ be our $n\times p$ data matrix (after using the EM algorithm) and $\m{A}$ be an $n\times p$ matrix of $0$'s and $1$'s, where $0$ represents a measured value in $\m{X}$ and $1$ represents a missing value. Define $\m{Z} = (\m{X}, \m{A})$, which is $n\times 2p$. We test the following hypothesis
\vspace{-3mm}
\[H_0:\m{\Sigma}_z = \left(\begin{array}{ll} \m{\Sigma}_{xx} & \m{O} \\ \m{O} & \m{\Sigma}_{aa} \end{array}\right),~\mathrm{or~equivalently}~H_0:\m{\Sigma}_{xa}=\m{O}\]
\vspace{-6mm}

\noindent which tests whether measured values and missingness are correlated. The test uses Wilks' $\Lambda$ statistic
\vspace{-3mm}
\[\Lambda=\frac{|\m{S}|}{|\m{S}_{xx}||\m{S}_{aa}|}\]
\vspace{-6mm}

\noindent where $\m{S}$ is the sample covariance matrix for $\m{Z}$ and $|\m{S}|$ is the determinant of $\m{S}$. This $\Lambda$ statistic is the measure of variability in $\m{S}$ due to $\m{S}_{xa}$ (the correlation between measured values and missing values). With more covariance in $\m{S}_{xa}$, the closer $\Lambda$ will be to $0$. Therefore, we reject $H_0$ for small values of $\Lambda$. Wilks' $\Lambda$ can be converted into an $F$-statistics, so we can calculate $p$-values.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c | rrr}
Region & \multicolumn{1}{l}{$\Lambda$}  & \multicolumn{1}{l}{$F$}    & \multicolumn{1}{l}{$p$-value} \\ \hline \hline
     2 & 0.202 & 1.182  & 0.186     \\
     4 & 0.147 & 0.903  & 0.658     \\
\end{tabular}
\caption{Wilks' $\Lambda$ test on $H_0:\m{\Sigma}_{xa}=\m{O}$}
\label{missing}
\end{center}
\end{table}
\vspace{-6mm}

Table \ref{missing} shows the results of the test. Since both $p$-values are above $0.05$, we do not reject the hypothesis that there is zero covariance between the measurements and whether or not a value is missing. Thus, the data are missing at random.
\vspace{-4mm}

\section*{Comparisons between regions}
\vspace{-4mm}
Because these regions neighbor each other, we are interested in testing whether these two regions' olives have the same fatty acid profile. Specifically, we perform the multivariate test $H_0: \m{\mu}_2 = \m{\mu}_4$, where $\m{\mu}_2$ is the population mean of region 2's fatty acids and $\m{\mu}_4$ is the population mean of region 4's fatty acids, a test that compares all variables simultaneously.

Because the standard two-sample comparison test assumes that both populations have the same covariance matrix, we compare the sample covariance matrices to address whether this assumption is justified. Let $\m{S}_2$ be the sample covariance matrix for region 2 and $\m{S}_4$ be the sample covariance matrix for region 4. Upon comparison, we find that the ratio between the diagonal components of $\m{S}_2$ and $\m{S}_4$ is as high as 8, and the ratio between the off-diagonal components reaches 480, suggesting that the covariance matrices for populations 2 and 4 are likely not equal. We, therefore, use the Nel and Van der Merwe test which relaxes the assumption that both populations must have equivalent covariance matrices.

For the Nel and Van der Merwe test, we compute $\m{S}_e = \frac{\m{S}_2}{n_2}+\frac{\m{S}_4}{n_4}$, $\overline{\m{x}}_1 = \frac{1}{n_2} \sum^{n_2}_{i=1} \m{x}_{2i}$, and $\overline{\m{x}}_4 = \frac{1}{n_4} \sum^{n_4}_{i=1} \m{x}_{2i}$ and compute ${T^*}^2 = (\overline{\m{x}}_2 - \overline{\m{x}}_4)' \m{S}^{-1}_e (\overline{\m{x}}_2 - \overline{\m{x}}_4)$ which approximately follows a $T^2_{p,\nu^*}$ distribution where $p=8$ acids and
\begin{equation*}
\nu^* = \frac{ \textrm{tr}\{\m{S}_e^2\} + (\textrm{tr}\{\m{S}_e\})^2}{\sum_{i=2,4} \frac{1}{n_i-1}\left[  \textrm{tr}\{(\m{S}_i/n_i)^2\} + (\textrm{tr}\{\m{S}_i/n_i\})^2  \right]}.
\end{equation*}
Note that $\mathrm{tr}(\cdot)$ is the trace function which sums up the diagonal elements of a matrix. Importantly, we can convert ${T^*}^2$ to an $F$-statistic to calculate $p$-values for our test using
$F= \frac{\nu^*-p+1}{\nu^* p} {T^*}^2$,
where $F$ follows an $F$ distribution with $p$ and $\nu^*-p+1$ degrees of freedom. For our test, we find ${T^*}^2=185.52$, $\nu^*=42.24$, and $F=19.35$. Since $p=8$, the $p$-value for this test is $8.990564\times10^{-11}$.  We, therefore, reject the hypothesis that both samples' olives have the same fatty acid profile. 

We then use the scaled discriminant function coefficients $\m{a}^*$ to identify fatty acids that are important in distinguishing the fatty acid profile of the two regions' olives.  Explicitly, $\m{a}^* = D^{1/2} \m{a}$ where

\[ \m{a}= \m{S}_e^{-1} (\overline{\m{x}}_2-\overline{\m{x}}_4),~~~
D^{1/2}= \begin{pmatrix}
\sqrt{s_{11}} &0&0&...&0 \\
0&\sqrt{s_{22}}&0&...&0 \\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&...&\sqrt{s_{pp}}
\end{pmatrix}, \] and $s_{ii}$ is the sample variance for the $i^{\textrm{th}}$ variable.  The relative magnitude of the coefficients in $\m{a}^*$ indicates how substantial the corresponding variable was in rejecting the multivariate test that $\overline{\m{x}}_2=\overline{\m{x}}_4$.  For this problem, $\m{a^*}=(-15.99,  -6.21, -17.71, -48.24, -25.43, 10.64, -16.87,  -5.02)'$ which corresponds to the following acids (palmitic, palmitoleic, stearic, oleic, linoleic, eicosanoic, linolenic, eicosenoic)$'$.  Because of their large magnitudes, oleic acid is the most important and linoleic acid is the second most important fatty acid in differentiating the olives from the different regions, and, in particular, both oleic acid and linoleic measurements were larger in region 4 relative to region 2. 
\vspace{-4mm}

\section*{Exclusion of linoleic and linolenic acid}
\vspace{-4mm}
We are also interested in answering whether linoleic and linolenic acids contribute any significant information beyond the information already available in the other 6 acids.  To perform this test, we calculate the Hotelling's $T^2$-statistic for the information we lose by excluding linoleic and linolenic acids, which is $T^2(\m{x} | \m{y}) =  (\nu - p) \frac{T^2_{p+q}-T^2_p}{\nu - T^2_p}$ where $\m{x}$ signifies linoleic and linolenic acids, $\m{y}$ is the other six fatty acids, $p=6$, $q=2$, $\nu$ is the degrees of freedom, $T^2_p$ depends on the other six acids, and $T^2_{p+q}$ is based on all acids.  Importantly, $T^2(\m{x} | \m{y})$ follows a Hotelling's $T^2_{q,\nu-p}$ distribution, and $\frac{\nu-p-q+q}{q} \frac{T^2_{p+q}-T^2_p}{\nu - T^2_p}$ follows a $F_{q,\nu-p-q+1}$ distribution, which enables us to calculate $p$-values. 

Because we do not assume that the covariance matrices of regions 2 and 4 are equal, we must augment $T^2(\m{x} | \m{y})$ to incorporate the adjustment used in the Nel and Van de Merwe test, ${T^*}^2(\m{x} | \m{y})=(\nu^* - p) \frac{{T^*}^2_{p+q}-{T^*}^2_p}{\nu - {T^*}^2_p}$.  We find, $T^{*2}(\m{x} | \m{y})=28.50$, which corresponds to an $F$-statistic of 13.86 and a $p$-value of $3.63\times10^{-5}$. We, therefore, reject the hypothesis that we can exclude linoleic and linolenic acid without losing significant information that distinguishes regions 2 and 4.
\vspace{-4mm}

\section*{Conclusion}
\vspace{-4mm}
In conclusion, we used the expectation-maximization (EM) algorithm to overcome the missing fatty acid measurements, a step we justify by our stating that missing values were missing at random in both regions. In addition, we failed to reject that samples from region 2 and region 4 for the given year differed from their historical means using Hotelling's $T^2$ tests. However, we rejected the hypothesis that regions 2 and 4's olives shared the same fatty acid profile using Nel and Van de Merwe's adjusted $T^2$ test.  Lastly, we used Hotelling's $T^2$ test on excluded data to reject the hypothesis that linoleic and linolenic acids do not contribute any significant information about how the regions differ beyond the information already available in the other 6 acids.

\section*{Appendix}

\subsection*{EM Algorithm output}
\begingroup
    \fontsize{7pt}{6pt}\selectfont
    
\begin{verbatim}

> ######## output em algorithm results
> 
> ### output for region 2
> round(max_2,digits=2)
      palmitic palmitoleic stearic   oleic linoleic eicosanoic linolenic eicosenoic
 [1,]  1315.00      139.00  230.00 7299.00   832.00      42.00     60.00      32.00
 [2,]  1321.00      136.00  217.00 7174.00   973.31      43.00     63.00      29.27
 [3,]  1359.00      115.00  246.00 7234.00   874.00      45.00     63.00      18.00
 [4,]  1378.00      111.00  272.00 7127.00   940.00      46.00     64.00      23.00
 [5,]  1295.00      109.00  245.00 7326.61   842.61      43.00     62.00      38.00
 [6,]  1275.00      121.00  270.05 7285.00   859.71      40.00     65.42      41.00
 [7,]  1336.00      120.00  334.35 7083.00   915.00      50.00     70.00      38.00
 [8,]  1309.00      122.00  241.00 7257.00   870.00      46.00     72.00      35.00
 [9,]  1340.00      114.00  189.00 7337.00   820.00      48.00     72.00      21.00
[10,]  1299.00      116.00  253.00 7309.00   823.00      40.00     69.00      27.00
[11,]  1221.00      122.12  221.00 7441.00   794.40      54.00     70.00      28.00
[12,]  1245.00       72.00  283.00 7395.00   829.00      44.00     67.00      28.00
[13,]  1285.00      129.00  244.00 7323.00   819.00      57.00     65.00      36.00
[14,]  1248.00      107.00  313.00 7299.00   840.00      46.00     66.00      33.00
[15,]  1356.00      106.00  236.00 7229.22   866.00      48.00     75.00      36.00
[16,]  1260.00      102.00  228.00 7354.00   870.00      49.00     64.00      28.00
[17,]  1261.00      121.00  312.00 7238.00   877.00      47.00     65.00      25.00
[18,]  1304.00      124.00  279.00 7160.00   928.00      48.00     61.00      37.00
[19,]  1344.00      117.00  310.92 7129.00   897.00      51.00     65.00      41.00
[20,]  1323.00       96.00  300.00 7351.00   757.00      47.00     54.00      26.00
[21,]  1292.00      117.00  215.00 7351.00   839.00      48.00     61.00      32.00
[22,]  1254.00      118.00  244.00 7394.00   786.00      46.00     70.72      24.00
[23,]  1320.93      131.00  259.00 7167.00   939.00      41.00     69.00      20.00
[24,]  1213.00      109.00  301.00 7261.00   925.00      47.00     64.98      31.00
[25,]  1341.68       98.00  351.00 7262.00   780.00      41.00     56.00      16.00
[26,]  1266.00       97.00  263.00 7435.00   743.00      47.94     69.00      29.00
[27,]  1298.00       99.00  312.90 7311.00   787.00      45.00     67.00      23.00
[28,]  1272.00      116.00  279.00 7258.00   872.00      50.02     72.00      27.00
[29,]  1278.00       87.00  332.00 7379.00   771.00      44.00     53.00      24.00
[30,]  1184.00      112.00  311.00 7391.00   819.00      48.00     57.00      28.00
[31,]  1382.00      110.00  268.00 7241.00   828.00      39.00     60.00      30.00
[32,]  1183.00      146.00  292.00 7580.00   618.00      38.00     51.00      23.00
[33,]  1261.00      153.00  219.00 7355.00   818.00      52.00     70.00      26.00
[34,]  1198.00      137.13  239.00 7639.00   633.00      27.00     55.00      19.00
[35,]  1225.00      134.00  232.00 7658.00   616.00      36.00     49.00      26.00
[36,]  1339.00      166.00  208.00 7190.00   923.00      40.00     69.00      25.00
[37,]  1132.00      157.00  240.00 7641.00   638.00      45.00     62.51      31.00
[38,]  1381.00      183.00  233.06 7385.00   609.00      47.00     70.00      25.00
[39,]  1409.00      128.00  257.00 7257.00   759.00      43.00     57.00      16.00
[40,]  1306.00      127.00  250.00 7257.59   869.00      47.00     68.00      24.00
[41,]  1372.00      120.00  250.00 7355.00   702.00      46.39     68.00      28.00
[42,]  1336.00      113.00  242.00 7293.00   855.00      38.00     60.00      23.79
[43,]  1401.00      135.90  238.00 7164.00   857.00      45.00     72.00      36.00
[44,]  1279.15      119.00  271.01 7323.88   823.00      40.00     62.00      41.00
[45,]  1432.00      152.00  272.20 7029.00   949.00      39.00     55.00      25.00
[46,]  1412.00      124.00  298.00 7182.00   790.00      45.00     68.00      28.00
[47,]  1369.80      147.00  291.00 7197.00   783.00      51.00     70.00      34.00
[48,]  1383.00      118.00  273.00 7282.00   749.06      45.00     68.00      29.00
[49,]  1283.00      102.00  263.00 7400.00   763.00      54.00     65.00      28.00
[50,]  1296.00      136.00  260.00 7380.00   764.47      48.00     51.00      18.00
[51,]  1287.00      108.00  287.00 7343.00   827.47      44.00     44.00      23.00
[52,]  1351.00      159.00  296.00 7229.00   810.00      36.00     60.00      22.00
[53,]  1241.00       97.00  268.00 7499.00   709.00      43.90     69.00      36.00
[54,]  1267.00      101.00  300.00 7230.00   898.00      74.00     65.00      34.00
[55,]  1235.00      138.00  252.00 7322.00   861.00      54.00     66.00      36.00
[56,]  1255.00      103.00  223.00 7404.65   848.00      47.00     56.00      26.72

 ### output for region 4
> round(max_4,digits=2)
      palmitic palmitoleic stearic   oleic linoleic eicosanoic linolenic eicosenoic
 [1,]  1222.00      133.00  227.00 7425.00      824      36.00     69.00      35.00
 [2,]  1639.00      172.00  331.00 6510.00     1124      46.00     91.00      32.00
 [3,]  1345.00      133.00  272.00 6801.00     1194      48.00     83.00      37.00
 [4,]  1339.00      170.00  275.00 6838.00     1060      46.00     88.00      43.00
 [5,]  1194.00      135.00  263.00 7277.00      889      44.00     95.00      41.00
 [6,]  1112.00       68.00  395.40 7770.00      448      52.00     69.00      39.35
 [7,]  1222.00       70.00  329.00 7605.00      566      48.00     67.00      38.66
 [8,]  1136.00       72.00  341.00 7616.00      661      49.00     65.00      34.09
 [9,]   985.91       21.96  277.00 7815.00      784      45.00     65.00      25.00
[10,]  1105.00       74.52  373.00 7714.00      532      51.00     68.00      37.00
[11,]  1096.12       79.00  305.00 7576.00      763      45.00     67.00      36.00
[12,]  1284.00       93.00  265.00 7235.00      893      43.00     77.00      46.00
[13,]  1120.00       69.00  275.05 7416.00      946      42.00     59.00      36.00
[14,]   916.00       52.00  281.00 7870.00      694      42.00     64.00      44.91
[15,]   905.00       49.00  288.00 7747.00      812      49.00     71.00      56.00
[16,]  1206.00       60.79  287.00 7329.00      935      44.00     74.00      42.00
[17,]  1457.00      182.00  267.00 7020.00      863      41.00     84.00      37.00
[18,]  1327.00      140.00  193.00 7328.00      823      36.00     87.00      35.00
[19,]  1303.00      100.00  251.00 7045.00     1049      40.00     86.00      40.00
[20,]  1444.00      175.00  259.00 6876.00     1027      34.00     78.00      32.00
[21,]  1505.00      243.00  226.00 6962.00      858      30.00     72.00      27.00
[22,]  1429.00      162.00  223.00 6917.00     1041      37.00     77.00      40.00
[23,]  1491.00      162.00  211.00 6994.00      928      37.00     97.00      38.00
[24,]  1393.00      128.00  211.00 7189.00      870      38.00     93.00      40.00
[25,]  1404.00      134.00  210.00 7110.00      923      40.00    101.00      43.00
[26,]  1222.00      130.00  214.00 7374.00      856      38.00     89.00      41.46
[27,]  1153.00       74.00  288.25 7623.94      705      42.00     64.00      32.00
[28,]  1169.00       76.00  307.00 7553.00      728      44.66     69.00      32.00
[29,]  1369.00      118.64  237.00 7375.00      775      39.00     70.00      15.00
[30,]   993.00       58.00  267.00 7730.00      773      41.00     66.35      44.00
[31,]   980.00       53.00  254.00 7719.00      815      44.00     69.00      47.00
[32,]   967.00       55.00  273.00 7692.00      833      43.49     63.00      41.78
[33,]  1128.00       73.00  354.00 7527.00      728      44.00     76.00      38.00
[34,]  1188.00       85.00  273.00 7445.00      814      44.00     73.00      42.00
[35,]  1257.00       95.00  247.00 7405.00      812      39.88     70.00      35.00
[36,]  1262.00       88.00  301.00 7471.00      704      43.00     71.00      33.03

\end{verbatim}

\subsection*{R Code used in analysis}

\begin{verbatim}
rm(list=ls())
# make the eigen values positive in covariance function
pd.sig = function(x){
  y = eigen(x, symmetric = TRUE)
  if (any(y$values < 0)){
    y$values[y$values < 0] = min(y$values[y$values > 0])
    return (y$vectors %*% diag(y$values) %*% t(y$vectors))
  } else {
    return (x)
  }
}

# expectation-maximization routine
em = function(x, tol = 0.01){
  miss = which(apply(x, 1, function(x) any(is.na(x))))
  if (length(miss) == 0)
    stop("Matrix x contains no missing values.")
  p = rep(list(0), length(miss))
  q = rep(list(0), length(miss))
  for (i in 1:length(miss)){
    q[[i]] = which(is.na(x[miss[i],]))
    p[[i]] = which(!is.na(x[miss[i],]))
  }
  mu = apply(x, 2, mean, na.rm = TRUE)
  sig = pd.sig(var(x, na.rm = TRUE))
  x[which(is.na(x))] = 0
  x.old = 2*x
  count = 0
  while (max(abs(x - x.old)) > tol){
    count = count + 1
    x.old = x
    # expectation
    for (i in 1:length(miss)){
      j = miss[i]
      x[j,q[[i]]] = mu[q[[i]]] + sig[q[[i]],p[[i]]] %*% 
        solve(sig[p[[i]],p[[i]]]) %*% (x[j,p[[i]]] - mu[p[[i]]])
    }
    # maximization
    mu = apply(x, 2, mean)
    sig = var(x)
  }
  cat("Iterations:",count,"\n")
  return (x)
}

#### test on region 2

olive2 <- as.matrix(read.table('oliver2a.txt',stringsAsFactors=FALSE,header=T))
n1 <- nrow(olive2)
p <- ncol(olive2)
max_2 <- em(olive2,tol=1e-6)
mu10 <- c(1300,120,265,7310,820,45,65,28)
S1 <- cov(max_2)
m1 <- apply(max_2,2,mean)
T2 <- n1 * t(m1 - mu10) %*% solve(S1) %*% (m1 - mu10)
F <- T2 * ((n1-1) - p + 1) / ((n1-1)*p)

1 - pf(F,p,n1-1 - p +1)

#### test on region 4

olive4 <- as.matrix(read.table('oliver4a.txt',stringsAsFactors=FALSE,header=T))
n2 <- nrow(olive4)
p <- ncol(olive4)

max_4 <- em(olive4,tol=1e-6)
mu20 <- c(1230,105,275,7360,830,41,75,38)
S2 <- cov(max_4)
m2 <- apply(max_4,2,mean)
T2 <- n2 * t(m2 - mu20) %*% solve(S2) %*% (m2 - mu20)
F <- T2 * ((n2-1) - p + 1) / ((n2-1)*p)

1 - pf(F,p,n2-1 - p +1)

#### two sample test problem 2

tr <- function(m) sum(diag(m))

Se <- S1/n1 + S2/n2

T2s <- t(m1-m2) %*% solve(Se) %*% (m1-m2)

num <- (tr(Se%*%Se) + (tr(Se))^2)
den <- 1/(n1-1)*( tr(S1/n1)^2 + tr((S1/n1)%*%(S1/n1)) ) + 1/(n2-1)*( tr(S2/n2)^2 + tr((S2/n2)%*%(S2/n2)) )
df.star <- num/den

F <- T2s * ((df.star) - p + 1) / ((df.star)*p)

1 - pf(F,p,df.star - p +1)

a <- solve(Se) %*% (m1 - m2)
a

astar <- diag(sqrt(diag(Se))) %*% a
c(astar) 


######  Excluding


olive2.ex <- max_2[,-c(5,7)]
p.ex <- ncol(olive2.ex)
q <- p-p.ex
S1.ex <- cov(olive2.ex)
m1.ex <- m1[-c(5,7)]


olive4.ex <- max_4[,-c(5,7)]
S2.ex <- cov(olive4.ex)
m2.ex <- m2[-c(5,7)]

Se.ex <- S1.ex/n1 + S2.ex/n2

T2sp <- t(m1.ex-m2.ex) %*% solve(Se.ex) %*% (m1.ex-m2.ex)

T_ex <- (df.star - p.ex) * (T2s - T2sp)/(df.star+ T2sp)

F_ex <-(df.star-p.ex-q+1)/q *(T2s - T2sp)/(df.star+ T2sp)
F_ex
1 - pf(F_ex,q,df.star - p.ex-q +1)

##### Missing Data

miss.mat2 <- 1*is.na(olive2)
miss.mat4 <- 1*is.na(olive4)

lam.to.f = function(lam, p, nu.h, nu.e){
    w = nu.e + nu.h - 0.5*(p+nu.h+1)
    t = sqrt((p^2 * nu.h^2 - 4)/(p^2 + nu.h^2 - 5))
    df1 = p * nu.h
    df2 = w*t - 0.5*(p*nu.h -2)
    F.stat = (lam^(-1/t) - 1) * df2/df1
#   return (c("F"=F.stat, "crit"=qf(1-0.95, df1, df2, lower.tail = FALSE)))
    return (c("F"=F.stat, "p-val"=pf(F.stat, df1, df2, lower.tail = FALSE)))
    }
des2 <- cbind(max_2,miss.mat2)
des4 <- cbind(max_4,miss.mat4)
des4 = des4[,-13]

S2 = var(des2)
R2 = cor(des2)
(lam2 = det(S2) / (det(S2[1:8,1:8]) * det(S2[9:16,9:16])))
lam.to.f(lam2, 8, 8, n1 - 1 - 8)
S4 = var(des4)
R4 = cor(des4)
(lam4 = det(S4) / (det(S4[1:8,1:8]) * det(S4[9:15,9:15])))
lam.to.f(lam4, 8, 7, n2 - 1 - 7)

######## output em algorithm results

### output for region 2
round(max_2,digits=2)

### output for region 4
round(max_4,digits=2)

\end{verbatim}
\endgroup

\end{document}
