\section{Conclusion}

Trees are a means of handling non-linear predictor spaces by partitioning the space into easily interpretable, non-overlapping regions. The high variance of trees are dealt with by random forests which grow many decorrelated, bootstrapped trees and average the predictions. Random forests allow us to assess the importance of variables by measuring the mean decrease in node impurity caused by the variable's split in the predictor space.

The method was applied to a classification problem on movie preferences. We were able to determine which of those variables were important and which were not in how the subject classified the movies. We also learned that some classes were not well explained by the model, having high error rates. Though we lose some interpretability when using random forests, we can still get an idea of how the variables are being divided by looking at one (or more) individual trees.
