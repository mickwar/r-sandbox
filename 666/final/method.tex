\section{Methods}

Random forests are constructed using an ensemble of trees from bootstrap samples. In this section I describe how to grow random forests and the rationale behind them.

\subsection{Growing a single tree}

Tree-based methods involve dividing the predictor variables, or feature space, $X_1,\ldots,X_p$ into $J$ distinct and non-overlapping regions $R_1,\ldots,R_J$. These regions may be any shape but are most often hyperrectangles formed by recursive binary splitting. The resulting regions are called terminal nodes, or leaves, and contain a subset of observations not found in any other terminal node.

Let $t$ be the set of observations within the terminal node being considered for splitting and $s=(v, c)$ denote the variable $v$ and cutoff $c$ as the corresponding split in the predictor space. That is, at split $s$ we separate the observations $i\in t$ into ``left'' and ``right'' groups, $t_L=\{i:x_{vi}\leq c, i\in t\}$ and $t_R=\{i:x_{vi} > c, i\in t\}$. Then grow a tree according the following algorithm:

\begin{enumerate}
\item Have every observation begin in the same node.
\item For every terminal node $t$ and every split $s$ compute $\phi(s, t)$.
\item Create two new terminal nodes $t_L$ and $t_R$ at split $s$ when $\phi(s, t)$ is optimized. The split node $t$ is now termed a parent node.
\item Repeat steps 2 and 3 until some stopping rule is met; for example, until no node contains more than five observations.
\end{enumerate}

The function $\phi(s, t)$ is some optimization function that results in the most homogenous children nodes. Homogeneity is assessed via some impurity measure. In the regression case we may let
\begin{eqnarray}
\phi(s, t) = SS(t) - SS(t_L) - SS(t_R)
\label{reg_phi}
\end{eqnarray}
and would thus want to maximize $\phi(s, t)$ for choosing the next split. Pure nodes, those containing closely related observations, will have minimum sums of squares $SS(t_L) + SS(t_R)$. The multivariate analog of sums of squares is given by
\begin{eqnarray}
SS(t) = \sum_{i\in t}(y_i-\mu(t))^\top V^{-1} (y_i-\mu(t))
\label{reg_ss}
\end{eqnarray}
where the response $y_i$ is a vector of length $q$, $\mu(t)$ is the mean response of every observation in node $t$, and the $q\times q$ matrix $V$ is a weighting ``covariance'' matrix. When $V=I_q$, the impurity of a node is the sum of squared Euclidean distances. When $q=1$ and $V=1$, (\ref{reg_ss}) reduces to $SS(t) = \sum_{i\in t}(y_i-\mu(t))^2$, the univariate sums of the squares.

For $K$ categorical responses, a common impurity measure is the Gini index defined by
\begin{eqnarray}
G(t) = \sum_{k=1}^K\hat{p}_{tk}(1-\hat{p}_{tk})
\label{cla_gini}
\end{eqnarray}
where $\hat{p}_{tk}$ is the proportion of observations in class $k$ found in node $t$. Smaller $G(t)$ indicate purer nodes, and $G(t)$ is close to zero when all of the $\hat{p}_{tk}$'s are close to zero or one. Growing trees using the Gini index would maximize $\phi(s, t)=G(t) - G(t_L) - G(t_R)$.

Alternatives to the Gini index include the classification error rate, using the most common class in the region as the prediction, and cross-entropy defined as $D(t) = -\sum_{k=1}^K\hat{p}_{tk}\mathrm{log}(\hat{p}_{tk})$. The Gini index and cross-entropy very similar numerically and both emphasize node purity, whereas classification error rate emphasizes prediction accuracy.




\subsection{Bootstrapping}


\subsection{Random forests}

For correlated random variables:

\[ \mathrm{Var}\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n^2}\sum_{i=1}^n\mathrm{Var}(X_i) + \frac{1}{n^2}\sum_{i=1}^{n-1}\sum_{j=i+1}^n\mathrm{Cov}(X_i,X_j) \]

\subsection{Variable importance}
