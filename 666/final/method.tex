\section{Method}

Random forests can be used for both regression and classification problems. I will focus on regression (continuous response), but the methods apply just as well to the classification case (categorical response).

\subsection{Growing a tree}

Tree-based methods involve dividing the predictor variables, or feature space, $X_1,\ldots,X_p$ into $J$ distinct and non-overlapping regions $R_1,\ldots,R_J$. Predictions for region $j$ are typically based on the mean (regression) or mode (classification) of the responses in $R_j$. This allows for easily interpretable results 

When the response is categorical, we take the predictive value to be that category having the highest number of observations found in the region, i.e. the mode.

Trees are grown by repeatedly splitting the feature space such that some function is optimized. For regression, the residual sums of squares is typically used. In the univariate case,
\[ RSS=\sum_{j=1}^J\sum_{i\in R_j}(y_i - \hat{y}_{R_j})^2 \]



For small $p$, taking $F$ random linear combinations of $L$ variables to define new inputs can lead to an increase in strength but higher correlation \citep{breiman:2001}. \cite{segal:2011} \cite{death:2002}

\subsection{Bootstrapping}


\subsection{Random forests}


\subsection{Variable importance}
