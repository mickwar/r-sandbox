\documentclass[mathserif, 11pt, t]{beamer}

\let\Tiny=\tiny
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{bm}
\geometry{vmargin=0.5in}

%colors
\definecolor{green}{rgb}{0.05, 0.9, 0.05}
\definecolor{bluegreen}{rgb}{0.05, 0.6, 0.2}

%commands
\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\m}[1]{\mathbf{\bm{#1}}}
\renewcommand{\frametitle}[1]{\bigskip\textcolor{green}{%
    \LARGE{\textbf{#1}}}\vspace{0.15cm}\newline}
\renewcommand{\subtitle}[1]{\vspace{0.45cm}\textcolor{bluegreen}{
    {\textbf{#1}}}\vspace{0.15cm}\newline}
\newcommand{\tlb}[1]{\large{\textbf{#1}}}

%slide colors
\pagecolor{green!40}

\begin{document}

% Title frame
\begin{center}
\ \\ [-0.5in]
\vfill
\bigskip
\bigskip
\bigskip
\bigskip
\bigskip

\begin{LARGE}
Multivariate Random Forests
\end{LARGE}
\vfill
\center{
Mickey Warner
}
\vfill
5 December 2014
\bigskip
\bigskip
\bigskip
\vfill
\ \\ [-0.5in]
\end{center}

\begin{frame}
\subtitle{Tree-based methods}
Decision trees:
\begin{itemize}[label={$\cdot$}]
\item Divide the predictor space into distinct, non-overlapping regions
\item Are easy to interpret trees
\item Are used for either regression or classification
\item Handle non-linear predictor spaces
\end{itemize}
\end{frame}

\begin{frame}
\subtitle{Comparison of trees and linear regression}
\begin{center}
\includegraphics[scale=0.20]{../figs/compare_tree_lm.pdf}
\end{center}
\end{frame}

\begin{frame}
\subtitle{Recursive binary splitting}
Define an optimization function $\phi(s, t)$
\begin{itemize}[label={$\cdot$}]
\item $s$ denotes a split location
\item $t$ indexes the observations in a terminal node
\end{itemize}
\bigskip

One choice: $\phi(s, t) = g(t) - g(t_L) - g(t_R)$
\begin{itemize}[label={$\cdot$}]
\item $t_L$ and $t_R$ index left and right nodes of a split $s$
\end{itemize}
\bigskip

Regression: $g(t) = SS(t) = \sum_{i\in t}(y_i-\mu(t))^\top V^{-1} (y_i-\mu(t))$
\bigskip

Classification: $g(t) = \sum_{k=1}^K\hat{p}_{tk}(1-\hat{p}_{tk})$, for $K$ classes
\bigskip

Repeatedly split terminal nodes until a stopping rule is met
\end{frame}

\begin{frame}
\subtitle{A tree!}
\begin{center}
\includegraphics[scale=0.20]{../figs/ex_tree.pdf}
\end{center}
\end{frame}

\begin{frame}
\subtitle{Random forests}
Grow $T$ trees where at each node a random $m\ll p$ predictors are used to split
\bigskip

Choices of $m$: $p/3$, $\sqrt{p}$, and $\log_2p + 1$.
\bigskip

For the (unpruned) trees, use bootstrap samples
\bigskip

Average the predictions from each tree for each observation
\end{frame}

\begin{frame}
\subtitle{Variable importance}
Some interpretation is lost when using forests over trees
\bigskip

Measure a variables importance by it's mean decrease of $g(t)$ averaged across all trees
\bigskip

Important variables will provide the most separation and hence largest decrease in $g(t)$.
\end{frame}

\begin{frame}
\subtitle{Example: movie preferences}
An unnamed participant (http://instagram.com/benbhansen) answered two questions about movies he has seen:
\begin{itemize}[label={$\cdot$}]
\item Did you like the movie?
\item Did you think the movie was well-executed?
\end{itemize}
\bigskip

Very few movies were liked, but not well-executed, so there are $K=3$ classes, labeled as follows:
\smallskip

~~1. Liked and well-executed
\smallskip

~~3. Disliked, but well-executed
\smallskip

~~4. Disliked and not well-executed
\end{frame}

\begin{frame}
\subtitle{Example: movie preferences}
Movie attributes ($p=9$ variables) were recorded:
\begin{itemize}[label={$\cdot$}]
\item Rotten Tomatoes critic score (\texttt{rot\_crit})
\item Kids in Mind ratings (\texttt{kids\_S}, \texttt{kids\_V}, \texttt{kids\_P})
\item MPAA rating (\texttt{G}, \texttt{PG}, \texttt{PG13})
\item Year of release (\texttt{year})
\item Live action (\texttt{live\_action})
\end{itemize}
\bigskip

We grow $T=500$ trees with $m=\sqrt{9}=3$.
\end{frame}

\begin{frame}
\subtitle{Group error rates}
\begin{center}
\includegraphics[scale=0.28]{../figs/forest_error.pdf}
\end{center}
\end{frame}

\begin{frame}
\subtitle{Variable importance and a tree}
\begin{center}
\includegraphics[scale=0.25]{../figs/importance2.pdf}
\end{center}
\end{frame}

\begin{frame}
\subtitle{Conclusion}
Random forests can dramatically decrease error rates
\bigskip

Predictions are more stable
\bigskip

For this example, some groups were poorly classified; other models may be more appropriate
\bigskip

The participant is a Mormon
\bigskip

\end{frame}


\end{document}
