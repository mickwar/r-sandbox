\section{Introduction} % and motivating example

A \emph{tree} is a convenient tool for making decisions or predictions based on partitioning the feature space. The mechanism for dividing the space is typically \emph{recursive binary splitting} which considers all possible splits at every variable, whether categorical or continuous, and determines which of those splits optimizes some criterion. Splits are made repeatedly until a stopping rule is met. The resulting partitions are called \emph{terminal nodes}, or \emph{leaves}, and represent non-overlapping hyperrectangles. Predictions for future observables are often made by using either the mean or mode of the observations within a terminal node.

To illustrate the usage of a tree, we consider the \texttt{Hitters} dataset from the \texttt{ISLR} package in R. The dataset contains performance measurements on $322$ baseball players as well as their \texttt{Salary} in 1,000's of dollars. We \emph{grow a tree} by partitioning the variables \texttt{Hits} and \texttt{Years} into three regions (see Figure \ref{tree1}). Predictions are made based on the mean $\log(\mathtt{Salary})$ for each region. Hence, we predict players in $R_1$ (having less than 4.5 years of experience) will have a mean salary of $\$1,000\times e^{5.107}=\$165,174.10$, those in $R_2$ (more than 4.5 years experience, but less than 117.5 hits) will have mean salary \$402,622.70, and those in $R_3$ (more than 4.5 years experience, more than 117.5 hits) with mean salary \$845,560.70. %Notice the simple nature of interpretation: we merely had to determine to which region an observation belongs to make a prediction.

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{figs/ex_tree.pdf}
\caption{Left: A regression tree with three terminal nodes, or leaves. Right: The divisions in the predictor space where each $R_j$ represents one of the leaves. Using two variables (\texttt{Years} and \texttt{Hits}) from the \texttt{Hitters} dataset in the R package \texttt{ISLR}. The response is $\log(\mathtt{Salary})$.}
\label{tree1}
\end{center}
\end{figure}

For any given data set, a tree will have high variance. That is, should we split the data in half and grow trees on each subset, we could have very different results. Hence, the predictive accuracy for a single tree is often poor. To resolve this issue, we use random forests. The basic concept behind a random forest is to grow a large number of trees based on a boostrap sample of the data and then average the predictions. This will cause a reduction in variance, just as the variance of a mean of i.i.d. random variables is reduced: $\mathrm{Var}(\overline{X}) = \sigma^2/n$.

In section 2, I give an overview of the methods used in constructing random forests. I omit the more complicated theoretical background for random forests and focus primarily on methodology. For a more theoretical treatment on random forests, see \cite{breiman:2001} and references therein. Section 3 contains a movie preference classification example.
