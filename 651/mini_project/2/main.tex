\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[font=footnotesize]{caption}
\usepackage{subcaption}

\newcommand{\m}[1]{\mathbf{\bm{#1}}}
\newcommand{\R}{I\hspace{-4.4pt}R}

\begin{document}

\noindent Mickey Warner

\noindent Stat 651 -- Mini Project 2

\section{Rejection sampling}

\subsection{Likelihood}

\noindent The measurements we are analyzing are teacher ratings (averaged across students) for a sample of faculty members. It seems reasonable to assume that the university hires its faculty according to similar standards and expectations. We thus assume each observation comes from the same distribution.
\bigskip

\noindent Further, since we are working with means we appeal to the Central Limit Theorem and propose the truncated normal (the data is bounded from 1 to 7) as the distribution of a single observation. This has density

\begin{eqnarray*}
f_*(y|\mu,\sigma^2,a,b) = \frac{\frac{1}{\sigma}\phi(\frac{y-\mu}{\sigma})}{\Phi(\frac{b-\mu}{\sigma})-\Phi(\frac{a-\mu}{\sigma})} & -\infty < \mu < +\infty; & \sigma > 0; \\
& -\infty \leq a < b \leq +\infty; & a < y < b,
\end{eqnarray*}

\noindent where $\phi(\cdot)$ is the density of the standard normal distribution, $\Phi(\cdot)$ is the c.d.f. of the standard normal, and $a$ and $b$ are the lower and upper bounds. We fix $a=1$ and $b=7$ because of our knowledge of the data. The likelihood function is

\[L(\m{y}|\m{\theta}) = \prod_{i=1}^n f_*(y_i|\m{\theta}) \]

\noindent for $\m{y}=(y_1,\ldots,y_n)^\top$ and $\m{\theta}=(\mu, \sigma^2)$ with $n=23$.

\subsection{Prior on $(\mu, \sigma^2)$}

\noindent We assume $\mu$ and $\sigma^2$ are independent so we may write

\[ \pi(\mu, \sigma^2) = \pi(\mu)\pi(\sigma^2). \]

\noindent This will simplify computations and there isn't any evidence to suggest non-independence nor to necessitate the use of a hierarchical specification for $\mu$ and $\sigma^2$.
\bigskip

\noindent We let $\mu\sim\mathcal{N}(5, 10^2)$ and $\sigma^2\sim\mathcal{IG}(2.5, 1.5)$ where the inverse gamma has the following parametrization

\[ \pi(\sigma^2) = \frac{b^a}{\Gamma(a)}(\sigma^2)^{-a-1}\exp(-b/\sigma^2);~~~ a,b,\sigma^2>0. \]

\noindent The parameter choice for $\mu$ result in a distribution centered around 5, which is to say we think on average the faculty would have a score of 5. The prior on $\sigma^2$ has a mean of 1, variance of 1.5, and the central 95\% density interval is $(0.234, 3.612)$. These both represent fairly non-informative priors, allowing the data to do more of the talking.

\subsection{The envelope distribution}

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/contour.pdf}
    \end{center}
    \caption{Contour plot of the unnormalized posterior. Note the slight covariance between $\mu$ and $\sigma^2$.}
    \label{contour}
\end{figure}


\noindent Figure \ref{contour} shows contours of the unnormalized joint posterior. We seek a distribution that will capture the covariance between $\mu$ and $\sigma^2$ and not approach $0$ in the tails too quickly or too slowly so as to have acceptable efficiency.
\bigskip

\noindent We will use the multivariate $t$ distribution, denoted $t_\nu(\m{\mu}, \m{\Sigma})$, as our envelope, which has density

\begin{eqnarray*}
W(\m{x}) = \frac{\Gamma[(\nu+p)/2]\nu^{-p/2}\pi^{-p/2}|\m{\Sigma}|^{-1/2}}{\Gamma(\nu/2)[1+\frac{1}{\nu}(\m{x}-\m{\mu})^\top\m{\Sigma}^{-1}(\m{x}-\m{\mu})]^{(\nu+p)/2}}; && \m{x}, \m{\mu} \in \R^p; \nu>0; \\
&& \m{\Sigma}~\mathrm{pos.~def.~in~} \R^{p\times p}
\end{eqnarray*}

\noindent where, after trial and error, we fix $\nu = 4$, $\m{\Sigma} = \left(\begin{array}{ll} 0.0522 & 0.0370 \\ 0.0370 & 0.0501 \\ \end{array}\right)$, and $\m{\mu}=(5.78, 0.31)^\top$ which is about the mode of the posterior. There are $p=2$ parameters. The log of this distribution evaluated at a grid of points $(\mu_i, \sigma^2_j)$ is shown in Figure \ref{envelope}. The unnormalized posterior $g(\m{\theta})$ is also evaluated at the same grid. We calculate $G=\max_i\frac{g(\mu_i,\sigma^2_j)}{W(\mu_i,\sigma^2_j)}$ and let $g^\star(\m{\theta}) = g(\m{\theta})/G$. We see in Figure \ref{envelope} that $W(\m{\theta})\geq g^\star(\m{\theta})$ for all $\m{\theta}$.
\bigskip

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{figs/env1.pdf}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{figs/env2.pdf}
    \end{subfigure}
    \caption{The logarithm of the scaled, unnormalized joint posterior (blue) and logarithm of multivariate $t$ envelope (green). Two different angles are shown.}
    \label{envelope}
\end{figure}

\noindent Though we only use a few set of points to calculate $G$, we are confident that $W$ dominates $g^\star$ due to the heavy tails in the multivariate $t$. Should $g^\star$ exceed $W$, this may be in a near-zero probability region, so we don't expect to have issues with this envelope distribution.
\bigskip

\noindent Drawing from the multivariate $t$ is simple enough. If $\m{y}\sim\mathcal{N}_p(\m{0},\m{\Sigma})$, $u\sim\chi^2_\nu$, and $\m{\mu}\in\R^p$, then $\m{x}=\m{\mu}+\m{y}\sqrt{\nu/u}$ is $t_\nu(\m{\mu},\m{\Sigma})$. Some draws will be negative for $\sigma^2$. This does not concern us since $g^\star$ is defined to be $0$ outside its support; however, we do lose some efficiency as a result.

\subsection{Posterior distribution}

\noindent We now be explicit on the form of $g(\m{\theta}) \propto \pi(\m{\theta}|\m{y})$.

\begin{eqnarray*}
\pi(\m{\theta}|\m{y}) &\propto& L(\m{y}|\mu, \sigma^2)\pi(\mu, \sigma^2) \\
&\propto& L(\m{y}|\mu, \sigma^2)\pi(\mu)\pi(\sigma^2) \\
&\propto& \prod_{i=1}^n f_*(y_i|\mu,\sigma^2)\times \exp\left(-\frac{(\mu-5)^2}{2\cdot10^2}\right) \times (\sigma^2)^{-2.5-1}\exp\left(-\frac{1.5}{\sigma^2}\right)
\end{eqnarray*}

\noindent where $f_*(\cdot)$ is given above and $g(\m{\theta})$ is equal to the right-hand side of the equation. A bivariate contour plot of $g(\m{\theta})$ is given in Figure \ref{contour}.

\subsection{Moment calculations}

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/draws.pdf}
    \end{center}
    \caption{A sample of $10,000$ draws from $\pi(\mu,\sigma^2|\m{y})$ overlain by contours.}
    \label{draws}
\end{figure}

\noindent We run the rejection algorithm $3,500,000$ times and obtain 1,220,000 draws (about 35\% efficiency) from the joint posterior distribution. The algorithm takes about 7 minutes to run. A sample of the draws are plotted in Figure \ref{draws}. Using these draws we make the following calculations:

\begin{eqnarray*}
\mathrm{E}(\mu,\sigma^2|\m{y}) &=& (5.808,~0.393) \\
\mathrm{Var}(\mu, \sigma^2|\m{y}) &=& \left(\begin{array}{ll} 0.0257 & 0.0112 \\ 0.0112 & 0.0275 \\ \end{array}\right) \\
\sqrt{\mathrm{Var}(\mu, \sigma^2|\m{y})} &=& (0.160,~0.165)
\end{eqnarray*}

\noindent The diagonal elements of $\mathrm{Var}(\mu, \sigma^2|\m{y})$ are the posterior variances of $\mu$ and $\sigma^2$ and the off-diagonal is the covariance.

\subsection{Posterior predictive distribution}

\noindent The predictive distribution is shown in Figure \ref{prediction}. We calculate the probability of a randomly selected teacher scoring 5 or more on their next evaluation at $0.897$.

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/pred.pdf}
    \end{center}
    \caption{Posterior predictive distribution.}
    \label{prediction}
\end{figure}

\section{Importance sampling}

\noindent For our importance function, $I(\mu, \sigma^2)$, we use the same multivariate $t$ distribution that we used for our envelope distribution in the rejection sampling. This function will do well since it has a similar shape as $g$, the unnormalized posterior, and has heavy tails. However, we suffer from the same problem as before where we may draw values from $I$ that have zero probability in $g$. This results in less than optimal accuracy for the amount of draws we will do.

\subsection{Moment calculations}

\begin{eqnarray*}
\mathrm{E}(\mu,\sigma^2|\m{y}) &=& (5.808,~0.393) \\
\mathrm{Var}(\mu, \sigma^2|\m{y}) &=& (0.026,~0.027) \\
\sqrt{\mathrm{Var}(\mu, \sigma^2|\m{y})} &=& (0.161,~0.165)
\end{eqnarray*}

\subsection{Estimation of the normalizing constant}

\noindent With importance sampling we can get an estimate of the normalizing constant
\[ \int_0^{+\infty}\int_{-\infty}^{+\infty} L(\m{y}|\mu,\sigma^2)\pi(\mu)\pi(\sigma^2)d\mu d\sigma^2 \]
which is the denominator in the posterior distribution. It is estimated by
\[ \hat{c} = \frac{1}{M}\sum_{i=1}^M \frac{g(\mu_i, \sigma^2_i)}{I(\mu_i,\sigma^2_i)} = 3.838\times 10^{-10} \]
where $(\mu_i, \sigma^2_i)$ are random draws from $I$, the multivariate $t$ distribution.
\bigskip

\noindent We may interpret this normalizing constant as the value necessary to make $g$ integrate to one. That is, $\lim_{M\rightarrow\infty} \int\cdots\int g(\m{\theta})/\hat{c}d\m{\theta} = 1$. But note that this $g$ as given in Section 1.4 and may still contain constants not removed from the posterior (there are many possibilities of unnormalized posteriors, depending on how much one wants to cancel out all of the terms beside the integral in the denominator). So $\hat{c}$ must be interpreted as the integral of the $g$ used in the importance sampling.

\section{Approach comparison}

\subsection{As computational procedures}

\noindent Rejection sampling is a sampling method which guarantees the draws we accept are from the target distribution. With these draws from the posterior distribution we have the option to calculate expectations, quantiles, and other features. In importance sampling, we are limited to the calculations of moments.
\bigskip

\noindent A major advantage of importance sampling over rejection sampling is the computation time. Depending on the efficiency of the algorithm, rejection sampling can take a very long time, and perhaps will vary from run to run. Importance sampling has a fixed number of iterations and will generally execute quicker than rejection sampling.

\subsection{With respect to the estimates produced}

\noindent Both rejection and importance sampling should produce the same estimates. The accuracy of estimates from the rejection algorithm depend on the number of accepted draws. The accuracy of importance sampling estimates depends on the choice of importance function and the number of runs. Poor importance functions will require more runs for good results or may even produce bad results, regardless of the number of runs.

\subsection{With respect to the difficulty of implementation}

\noindent Valid importance functions are much easier to obtain than valid envelope distributions. For an envelope distribution to be valid, it must be verified that the density dominates the unnormalized posterior for the entire parameter space. As seen in this paper, even in two dimensions finding a valid envelope is difficult, especially in light of the efficiency (note the obscure parameters in the multivariate $t$). I imagine that self-amputation is more pleasant than finding an envelope for more than two dimensions.
\bigskip

\noindent Both algorithms are fairly simple to actually code. Rejection sampling needs to be done in a loop to produce exactly the desired number of accepted draws. Importance sampling can be entirely vectorized. This is akin to kitten hugging; we all like them vectors.

\end{document}
