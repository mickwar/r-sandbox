\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{subcaption}
\usepackage[font=footnotesize]{caption}

\newcommand{\m}[1]{\mathbf{\bm{#1}}}
\newcommand{\R}{I\hspace{-4.4pt}R}

\begin{document}

\noindent Mickey Warner

\noindent Stat 651 -- Mini Project 2

\section{Rejection sampling}

\subsection{Likelihood}

\noindent The measurements we are analyzing are teacher ratings (averaged across students) for a sample of faculty members. It seems reasonable to assume that the university hires its faculty according to similar standards and expectations. We thus assume each observation comes from the same distribution.
\bigskip

\noindent Further, since we are working with means we appeal to the Central Limit Theorem and propose the truncated normal (the data is bounded from 1 to 7) as the distribution of a single observation. This has density

\begin{eqnarray*}
f_*(y|\mu,\sigma^2,a,b) = \frac{\frac{1}{\sigma}\phi(\frac{y-\mu}{\sigma})}{\Phi(\frac{b-\mu}{\sigma})-\Phi(\frac{a-\mu}{\sigma})} & -\infty < \mu < +\infty; & \sigma > 0; \\
& -\infty \leq a < b \leq +\infty; & a < y < b,
\end{eqnarray*}

\noindent where $\phi(\cdot)$ is the density of the standard normal distribution, $\Phi(\cdot)$ is the c.d.f. of the standard normal, and $a$ and $b$ are the lower and upper bounds. We fix $a=1$ and $b=7$ because of our knowledge of the data. The likelihood function is

\[L(\m{y}|\m{\theta}) = \prod_{i=1}^n f_*(y_i|\m{\theta}) \]

\noindent for $\m{y}=(y_1,\ldots,y_n)^\top$ and $\m{\theta}=(\mu, \sigma^2)$ with $n=23$.

\subsection{Prior on $(\mu, \sigma^2)$}

\noindent We assume $\mu$ and $\sigma^2$ are indepedent so we may write

\[ \pi(\mu, \sigma^2) = \pi(\mu)\pi(\sigma^2). \]

\noindent This will simplify computations and there isn't any evidence to suggest nonindependence nor to necessitate the use of a hierarchical specification for $\mu$ and $\sigma^2$.
\bigskip

\noindent We let $\mu\sim\mathcal{N}(5, 10^2)$ and $\sigma^2\sim\mathcal{IG}(2.5, 1.5)$ using the inverse gamma with the following parametrization

\[ \pi(\sigma^2) = \frac{b^a}{\Gamma(a)}(\sigma^2)^{-a-1}\exp(-b/\sigma^2);~~~ a,b,\sigma^2>0. \]

\noindent The hyperpriors on $\mu$ result in a distribution centered around 5, which is to say we think on average the faculty would have a score of 5. The prior on $\sigma^2$ has a mean of 1, variance of 1.5, and the central 95\% density interval is $(0.234, 3.612)$. These both represent fairly noninformative priors, allowing the data to do more of the talking.

\subsection{The envelope distribution}

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/contour.pdf}
    \end{center}
    \caption{Contour plot of the unnormalized posterior. Note the slight covariance between $\mu$ and $\sigma^2$.}
    \label{contour}
\end{figure}


\noindent Figure \ref{contour} shows contours of the unnormalized joint posterior. We seek a distribution that will capture the covariance between $\mu$ and $\sigma^2$ and not approach $0$ in the tails too quickly or too slowly so as to have acceptable efficiency.
\bigskip

\noindent We will use the multivariate $t$ distribution, denoted $t_\nu(\m{\mu}, \m{\Sigma})$, as our envelope, which has density

\begin{eqnarray*}
W(\m{x}) = \frac{\Gamma[(\nu+p)/2]\nu^{-p/2}\pi^{-p/2}|\m{\Sigma}|^{-1/2}}{\Gamma(\nu/2)[1+\frac{1}{\nu}(\m{x}-\m{\mu})^\top\m{\Sigma}^{-1}(\m{x}-\m{\mu})]^{(\nu+p)/2}}; && \m{x}, \m{\mu} \in \R^p; \nu>0; \\
&& \m{\Sigma}~\mathrm{pos.~def.~in~} \R^{p\times p}
\end{eqnarray*}

\noindent where, after trial and error, we fix $\nu = 4$, $\m{\Sigma} = \left(\begin{array}{ll} 0.0522 & 0.0370 \\ 0.0370 & 0.0501 \\ \end{array}\right)$, and $\m{\mu}=(5.78, 0.31)^\top$ which is about the mode of the posterior. There are $p=2$ parameters. The log of this distribution evaluated at a grid of points $(\mu_i, \sigma^2_j)$ is shown in Figure \ref{envelope}. The unnormalized posterior $g(\m{\theta})$ is also evaulated at the same grid. We calculate $G=\max_i\frac{g(\mu_i,\sigma^2_j)}{W(\mu_i,\sigma^2_j)}$ and let $g^\star(\m{\theta}) = g(\m{\theta})/G$. We see in Figure \ref{envelope} that $W(\m{\theta})\geq g^\star(\m{\theta})$ for all $\m{\theta}$.
\bigskip

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{figs/env1.pdf}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{figs/env2.pdf}
    \end{subfigure}
    \caption{The logarithm of the scaled, unnormalized joint posterior (blue) and logarithm of multivariate $t$ envelope (green). Two different angles are shown.}
    \label{envelope}
\end{figure}

\noindent Though we only use a few set of points to calculate $G$, we are confident that $W$ dominates $g^\star$ due to the heavy tails in the multivariate $t$. Should $g^\star$ exceed $W$, this may be in a near-zero probability region, so we don't expect to have issues with this envelope distribution.
\bigskip

\noindent Drawing from the multivariate $t$ is simple enough. If $\m{y}\sim\mathcal{N}_p(\m{0},\m{\Sigma})$, $u\sim\chi^2_\nu$, and $\m{\mu}\in\R^p$, then $\m{x}=\m{\mu}+\m{y}\sqrt{\nu/u}$ is $t_\nu(\m{\mu},\m{\Sigma})$. Some draws will be negative for $\sigma^2$. This does not concern us since $g^\star$ is defined to be $0$ outside its support; however, we do lose some efficiency as a result.

\subsection{Posterior distribution}

\noindent We now be explicit on the form of $g(\m{\theta}) \propto \pi(\m{\theta}|\m{y})$.

\begin{eqnarray*}
\pi(\m{\theta}|\m{y}) &\propto& L(\m{y}|\mu, \sigma^2)\pi(\mu, \sigma^2) \\
&\propto& L(\m{y}|\mu, \sigma^2)\pi(\mu)\pi(\sigma^2) \\
&\propto& \prod_{i=1}^n f_*(y_i|\mu,\sigma^2)\times \exp\left(-\frac{(\mu-5)^2}{2\cdot10^2}\right) \times (\sigma^2)^{-2.5-1}\exp\left(-\frac{1.5}{\sigma^2}\right)
\end{eqnarray*}

\noindent where $f_*(\cdot)$ is given above and $g(\m{\theta})$ is equal to the right-hand side of the equation. A bivariate contour plot of $g(\m{\theta})$ is given in Figure \ref{contour}.

\subsection{Moment calculations}

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/draws.pdf}
    \end{center}
    \caption{A sample of $10,000$ draws from $\pi(\mu,\sigma^2|\m{y})$ overlain by contours.}
    \label{draws}
\end{figure}

\noindent We run the rejection algorithm $3,500,000$ times and obtain 1,220,000 draws (about 35\% efficiency) from the joint posterior distribution. The algorithm takes about 7 minutes to run. A sample of the draws are plotted in Figure \ref{draws}. Using these draws we make the following calculations:

\begin{eqnarray*}
\mathrm{E}(\mu,\sigma^2|\m{y}) &=& (5.808,~0.393) \\
\mathrm{Var}(\mu, \sigma^2|\m{y}) &=& \left(\begin{array}{ll} 0.0257 & 0.0112 \\ 0.0112 & 0.0275 \\ \end{array}\right) \\
\sqrt{\mathrm{Var}(\mu, \sigma^2|\m{y})} &=& (0.160,~0.165)
\end{eqnarray*}

\noindent The diagonal elements of $\mathrm{Var}(\mu, \sigma^2|\m{y})$ are the posterior variances of $\mu$ and $\sigma^2$ and the off-diagonals are the covariances.

\subsection{Posterior predictive distribution}

\noindent The predictive distribution is shown in Figure \ref{prediction}. We calculate the probability of a randomly selected teacher scoring 5 or more on their next evaluation at $0.897$.

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/pred.pdf}
    \end{center}
    \caption{Posterior predictive distribution.}
    \label{prediction}
\end{figure}

\section{Importance sampling}

\noindent For our importance function, $I(\mu, \sigma^2)$, we use the same multivariate $t$ distribution that we used for our envelope distribution in the rejection sampling. This function will do well since it has a similar shape as $g$, the unnormalized posterior, and has heavy tails. However, we suffer from the same problem as before where we may draw values from $I$ that have zero probability in $g$. This results in less than optimal accuracy for the amount of draws we will do.

\subsection{Moment calculations}

\begin{eqnarray*}
\mathrm{E}(\mu,\sigma^2|\m{y}) &=& (5.808,~0.393) \\
\mathrm{Var}(\mu, \sigma^2|\m{y}) &=& (0.026,~0.027) \\
\sqrt{\mathrm{Var}(\mu, \sigma^2|\m{y})} &=& (0.161,~0.165)
\end{eqnarray*}

\subsection{Estimation of the normalizing constant}

\noindent With importance sampling we can get an estimate of the normalizing constant
\[ \int_0^{+\infty}\int_{-\infty}^{+\infty} L(\m{y}|\mu,\sigma^2)\pi(\mu)\pi(\sigma^2)d\mu d\sigma^2 \]
which is the denominator in the posterior distribution. It is estimated by
\[ \hat{c} = \frac{1}{M}\sum_{i=1}^M \frac{g(\mu_i, \sigma^2_i)}{I(\mu_i,\sigma^2_i)} = 3.838\times 10^{-10} \]
where $(\mu_i, \sigma^2_i)$ are random draws from $I$, the multivariate $t$ distribution.
\bigskip

\noindent We may interpret this normalizing constant as the value necessary to make $g$ intergrate to one. That is, $\lim_{M\rightarrow\infty} \int\cdots\int g(\m{\theta})/\hat{c}d\m{\theta} = 1$. But note that this $g$ as given in Section 1.4 and may still contain constants not removed from the posterior (there are many possibilities of unnormalized posteriors, depending on how much one wants to cancel out all of the terms beside the integral in the denominator). So $\hat{c}$ must be interpreted as the integral of the $g$ used in the importance sampling.

\section{Approach comparison}

\subsection{As computational procedures}

\subsection{With respect to the estimates produced}

\subsection{With respect to the difficulty of implementation}

\end{document}
