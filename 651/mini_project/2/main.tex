\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{subcaption}
\usepackage[font=footnotesize]{caption}

\newcommand{\m}[1]{\mathbf{\bm{#1}}}
\newcommand{\R}{I\hspace{-4.4pt}R}

\begin{document}

\noindent Mickey Warner

\noindent Stat 651 -- Mini Project 2

\section{Rejection Sampling}

\subsection{Likelihood}

\noindent The measurements we are analyzing are teacher ratings (averaged across students) for a sample of faculty members. It seems reasonable to assume that the university hires its faculty according to similar standards and expectations. We thus assume each observation comes from the same distribution.
\bigskip

\noindent Further, since we are working with means we appeal to the Central Limit Theorem and propose the truncated normal (the data is bounded from 1 to 7) as the distribution of a single observation. This has density

\begin{eqnarray*}
f_*(y|\mu,\sigma^2,a,b) = \frac{\frac{1}{\sigma}\phi(\frac{y-\mu}{\sigma})}{\Phi(\frac{b-\mu}{\sigma})-\Phi(\frac{a-\mu}{\sigma})} & -\infty < \mu < +\infty; & \sigma > 0; \\
& -\infty \leq a < b \leq +\infty; & a < y < b, \\
\end{eqnarray*}

\noindent where $\phi(\cdot)$ is the density of the standard normal distribution, $\Phi(\cdot)$ is the c.d.f. of the standard normal, and $a$ and $b$ are the lower and upper bounds. We fix $a=1$ and $b=7$ because of our knowledge of the data. The likelihood function is

\[L(\m{y}|\m{\theta}) = \prod_{i=1}^n f_*(y_i|\m{\theta}) \]

\noindent for $\m{y}=(y_1,\ldots,y_n)^\top$ and $\m{\theta}=(\mu, \sigma^2)$ with $n=23$.

\subsection{Prior on $(\mu, \sigma^2)$}

\noindent We assume $\mu$ and $\sigma^2$ are indepedent so we may write

\[ \pi(\mu, \sigma^2) = \pi(\mu)\pi(\sigma^2). \]

\noindent This will simplify computations and there isn't any evidence to suggest nonindependence nor to necessitate the use of a hierarchical specification for $\mu$ and $\sigma^2$.
\bigskip

\noindent We let $\mu\sim\mathcal{N}(5, 10^2)$ and $\sigma^2\sim\mathcal{IG}(2.5, 1.5)$ using the inverse gamma with the following parametrization

\[ \pi(\sigma^2) = \frac{b^a}{\Gamma(a)}(\sigma^2)^{-a-1}\exp(-b/\sigma^2);~~~ a,b,\sigma^2>0. \]

\noindent The hyperpriors on $\mu$ result in a distribution centered around 5, which is to say we think on average the faculty would have a score of 5. The prior on $\sigma^2$ has a mean of 1, variance of 1.5, and the central 95\% density interval is $(0.234, 3.612)$. These both represent fairly noninformative priors, allowing the data to do more of the talking.

\subsection{The Envelope Distribution}

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/contour.pdf}
    \end{center}
    \caption{Contour plot of the unnormalized posterior. Note the slight covariance between $\mu$ and $\sigma^2$.}
    \label{contour}
\end{figure}


\noindent Figure \ref{contour} shows contours of the unnormalized joint posterior. We seek a distribution that will capture the covariance between $\mu$ and $\sigma^2$ and not approach $0$ in the tails too quickly or too slowly so as to have acceptable efficiency.
\bigskip

\noindent We will use the multivariate $t$ distribution, denoted $t_\nu(\m{\mu}, \m{\Sigma})$, as our envelope, which has density

\begin{eqnarray*}
W(\m{x}) = \frac{\Gamma[(\nu+p)/2]\nu^{-p/2}\pi^{-p/2}|\m{\Sigma}|^{-1/2}}{\Gamma(\nu/2)[1+\frac{1}{\nu}(\m{x}-\m{\mu})^\top\m{\Sigma}^{-1}(\m{x}-\m{\mu})]^{(\nu+p)/2}}; && \m{x}, \m{\mu} \in \R^p; \nu>0; \\
&& \m{\Sigma}~\mathrm{pos.~def.~in~} \R^{p\times p}\\
\end{eqnarray*}

\noindent where, after trial and error, we fix $\nu = 4$, $\m{\Sigma} = \left(\begin{array}{ll} 0.0522 & 0.0370 \\ 0.0370 & 0.0501 \\ \end{array}\right)$, and $\m{\mu}=(5.78, 0.31)^\top$ which is about the mode of the posterior. There are $p=2$ parameters. The log of this distribution evaluated at a grid of points $(\mu_i, \sigma^2_j)$ is shown Figure \ref{envelope}. The unnormalized posterior $g(\m{\theta})$ is also evaulated at the same grid. We calculate $G=\max_i\frac{g(\mu_i,\sigma^2_j)}{W(\mu_i,\sigma^2_j)}$ and let $g^\star(\m{\theta}) = g(\m{\theta})/G$. We see in Figure \ref{envelope} that $W(\m{\theta})\geq g^\star(\m{\theta})$ for all $\m{\theta}$.
\bigskip

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{figs/env1.pdf}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{figs/env2.pdf}
    \end{subfigure}
    \caption{The logarithm of the scaled, unnormalized joint posterior (blue) and logarithm of multivariate $t$ envelope (green). Two different angles are shown.}
    \label{envelope}
\end{figure}

\noindent Though we only use a few set of points to calculate $G$, we are confident that $W$ dominates $g^\star$ due to the heavy tails in the multivariate $t$. Should $g^\star$ exceed $W$, this may be in a near-zero probability region, so we don't expect to have issues with this envelope distribution.
\bigskip

\noindent Drawing from the multivariate $t$ is simple enough. If $\m{y}\sim\mathcal{N}_p(\m{0},\m{\Sigma})$, $u\sim\chi^2_\nu$, and $\m{\mu}\in\R^p$, then $\m{x}=\m{\mu}+\m{y}\sqrt{\nu/u}$ is $t_\nu(\m{\mu},\m{\Sigma})$. Some draws will be negative for $\sigma^2$. This does not concern us since $g^\star$ is defined to be $0$ outside its support.

\subsection{Posterior Distribution}

\noindent We now be explicit on the form of $g(\m{\theta}) \propto \pi(\m{\theta}|\m{y})$.

\begin{eqnarray*}
\pi(\m{\theta}|\m{y}) &\propto& L(\m{y}|\mu, \sigma^2)\pi(\mu, \sigma^2) \\
&\propto& L(\m{y}|\mu, \sigma^2)\pi(\mu)\pi(\sigma^2) \\
&\propto& \prod_{i=1}^n f_*(y_i|\mu,\sigma^2)\times \exp\left(-\frac{(\mu-5)^2}{2\cdot10^2}\right) \times (\sigma^2)^{-2.5-1}\exp\left(-\frac{1.5}{\sigma^2}\right) \\ 
\end{eqnarray*}

\noindent where $f_*(\cdot)$ is given above and $g(\m{\theta})$ is equal to the right-hand side of the equation. A bivariate contour plot of $g(\m{\theta})$ is given in Figure \ref{contour}. Note that we do not remove any constant terms in $f_*(\cdot)$ so we can make use of \texttt{R} functions.

\subsection{Moment Calculations}

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/draws.pdf}
    \end{center}
    \caption{A sample of $10,000$ draws from $\pi(\mu,\sigma^2|\m{y})$ overlain by contours.}
    \label{draws}
\end{figure}

\noindent We run the rejection algorithm $3,500,000$ times and obtain 1,220,000 draws (about 35\% efficiency) from the joint posterior distribution. The algorithm takes about 7 minutes to run. A sample of the draws are plotted in Figure \ref{draws}. Using these draws we make the following calculations:

\begin{eqnarray*}
\mathrm{E}(\mu,\sigma^2|\m{y}) &=& (5.808,~0.393) \\
\mathrm{Var}(\mu, \sigma^2|\m{y}) &=& \left(\begin{array}{ll} 0.0257 & 0.0112 \\ 0.0112 & 0.0275 \\ \end{array}\right) \\
\sqrt{\mathrm{Var}(\mu, \sigma^2|\m{y})} &=& (0.160,~0.165) \\
\end{eqnarray*}

\subsection{Posterior Predictive Distribution}

\noindent The predictive distribution is shown in Figure \ref{prediction}. We calculate the probability of a randomly selected teacher scoring 5 or more on their next evaluation at $0.897$.

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.4]{figs/pred.pdf}
    \end{center}
    \caption{Posterior predictive distribution.}
    \label{prediction}
\end{figure}

\section{Importance Sampling}

\noindent Hereo this goes

\section{Approach Comparison}

\end{document}
