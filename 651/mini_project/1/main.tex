\documentclass[12pt]{article}

\usepackage{graphicx,float,units,breqn}
\usepackage[font=footnotesize]{caption}

\newcommand{\ybar}{\overline{y}}

\begin{document}

%\begin{Large}
\noindent Mickey Warner
%\bigskip

\noindent Stat 651
%\end{Large}
\bigskip

\section{The 2001 Season for Barry Bonds}

\subsection{Summary}

\noindent The following is a Bayesian analysis of home runs and at-bats for former baseball player Barry Bonds during his 2001 season with the San Francisco Giants.  We take a close look at his home run percentage and provide several estimates to better understand his performance that year.

\subsection{The Likelihood and Conjugate Prior}

The probability density function comes from the binomial distribution, where $y_i$ is the number of home runs and $n_i$ is the number of at-bats in game $i$.  The function is defined as:

%\[f_*(y_i|\theta)={n_i \choose y_i}\theta^{y_i}(1-\theta)^{n_i-y_i}, 0<\theta<1, n_i=0,1,\cdots, y_i=0,1,\cdots,n_i\]

\begin{eqnarray*}
f_*(y_i|\theta)={n_i \choose y_i}\theta^{y_i}(1-\theta)^{n_i-y_i}; && 0\leq\theta\leq1; \\
&& n_i=0,1,\cdots; \\
&& y_i=0,1,\cdots,n_i \\
\end{eqnarray*}

\noindent The likelihood function is:

\begin{eqnarray*}
L(\mathbf{y}|\theta) &=& \prod_{i=1}^n\left[{n_i \choose y_i}\theta^{y_i}(1-\theta)^{n_i-y_i}\right] \\
&=& \left[\prod_{i=1}^n{n_i \choose y_i}\right]\theta^{\Sigma y_i}(1-\theta)^{\Sigma n_i-\Sigma y_i} \\
\end{eqnarray*}

\noindent For the prior we will use a beta distribution with parameters $a$ and $b$, which has the pdf
\[\pi(\theta)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}\ \ \ \ \ 0<\theta<1;\ a>0;\ b>0 \]

\noindent Solving for the posterior distribution will show that we have a conjugate prior.  This is accomplished by multiplying the likelihood and the prior together and diving by the integral of that product.

\begin{eqnarray*}
\pi(\theta|\mathbf{y}) &=& \frac{\left[\prod_{i=1}^n{n_i \choose y_i}\right]\theta^{\Sigma y_i}(1-\theta)^{\Sigma n_i-\Sigma y_i}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}}{\int_0^1\left[\prod_{i=1}^n{n_i \choose y_i}\right]\theta^{\Sigma y_i}(1-\theta)^{\Sigma n_i-\Sigma y_i}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}d\theta}  \\
\\
&=& \frac{\theta^{\Sigma y_i}(1-\theta)^{\Sigma n_i-\Sigma y_i}\theta^{a-1}(1-\theta)^{b-1}}{\int_0^1\theta^{\Sigma y_i}(1-\theta)^{\Sigma n_i-\Sigma y_i}\theta^{a-1}(1-\theta)^{b-1}d\theta} \\
\\
&=& \frac{\theta^{a+\Sigma y_i-1}(1-\theta)^{b+\Sigma n_i-\Sigma y_i-1}}{\int_0^1\theta^{a+\Sigma y_i-1}(1-\theta)^{b+\Sigma n_i-\Sigma y_i-1}d\theta} \\
\end{eqnarray*}

\noindent Let $a^*=a+\Sigma y_i$ and $b^*=b+\Sigma n_i-\Sigma y_i$, then

\begin{eqnarray*}
\pi(\theta|\mathbf{y}) &=& \frac{\theta^{a^*-1}(1-\theta)^{b^*-1}}{\int_0^1\theta^{a^*-1}(1-\theta)^{b^*-1}d\theta} \\
&=& \frac{\theta^{a^*-1}(1-\theta)^{b^*-1}}{\frac{\Gamma(a^*)\Gamma(b^*)}{\Gamma(a^*+b^*)}\int_0^1\frac{\Gamma(a^*+b^*)}{\Gamma(a^*)\Gamma(b^*)}\theta^{a^*-1}(1-\theta)^{b^*-1}d\theta} \\
&=& \frac{\theta^{a^*-1}(1-\theta)^{b^*-1}}{\frac{\Gamma(a^*)\Gamma(b^*)}{\Gamma(a^*+b^*)}} \\
&=& \frac{\Gamma(a^*+b^*)}{\Gamma(a^*)\Gamma(b^*)}\theta^{a^*-1}(1-\theta)^{b^*-1} \\
\end{eqnarray*}

\noindent which is a beta distribution with parameters $a^*=a+\Sigma y_i$ and $b^*=b+\Sigma n_i-\Sigma y_i$.  Thus, we have a conjugate prior.

\subsection{Prior Parameters}

\noindent Values for the priors were chosen to be $a=5$ and $b=45$.  When looking at Barry Bonds' record from the beginning of his career, we can see that his home run per at-bat percent has been gradually increasing from about 5\% in 1986 to about 10\% in 2000.  With the chosen priors we would have a mean of $\frac{5}{5+45}=10\%$.  Though we must be careful to have enough flexibility in these priors to adjust for the 2001 season, while still building off the previous years.
\bigskip

\noindent With a relatively high standard deviation of $\sqrt{\frac{(5)(45)}{(5+45)^2(5+45+1)}}=0.042$ we are more likely to take into account his performance in earlier years.  However, since his record has been improving, we do not want too much weight into these priors.  Having too high of priors could pull down our estimate calculated from the posterior distribution.  Thus values such as $a=10$ and $b=90$ were not selected, even though that distribution has a mean of $10\%$.  So $a=5$ and $b=45$ seem to be an appropriate choice.
\bigskip


\subsection{Maximum Likelihood and Plots}

%\noindent The plot for a Beta distribution with parameters $\alpha=5$ and $\beta=45$.

%\begin{figure}[H]
%\begin{center}
%\includegraphics[scale=0.5]{figs/1priorplot.pdf}
%\caption{Prior distribution}
%\end{center}
%\end{figure}
%\bigskip
%\bigskip
%\bigskip

\noindent \emph{Parameters of the posterior distribution.}  In 2001, Barry Bonds was at-bat 476 times and hit 73 home runs.  Using this data to update our priors, we get posterior parameters of $a^*=5+73=78$ and $b^*=45+476-73=448$.
\bigskip

\noindent \emph{Derivation of the maximum likelihood estimator.}  First, take the logarithm of the likelihood.

\begin{eqnarray*}
\mathrm{log}L(\mathbf{y}|\theta)&=& \mathrm{log}\left(\left[\prod_{i=1}^n{n_i \choose y_i}\right]\theta^{\Sigma y_i}(1-\theta)^{\Sigma n_i-\Sigma y_i}\right) \\
&=& \mathrm{log}\left[\prod_{i=1}^n{n_i \choose y_i}\right]+\Sigma y_i\mathrm{log}\theta+(\Sigma n_i-\Sigma y_i)\mathrm{log}(1-\theta) \\
\end{eqnarray*}

\noindent Then take the derivative.  

\begin{eqnarray*}
\frac{d}{d\theta}\left[\mathrm{log}L(\mathbf{y}|\theta)\right] &=& 0 + \frac{\Sigma y_i}{\theta} - \frac{\Sigma n_i-\Sigma y_i}{1-\theta} \\
&=& \frac{\Sigma y_i}{\theta} - \frac{\Sigma n_i-\Sigma y_i}{1-\theta} \\
\end{eqnarray*}

\noindent Setting this equation equal to 0 and solving for $\hat{\theta}$ will give us our desired estimate.

\begin{eqnarray*}
0 &=& \frac{\Sigma y_i}{\hat{\theta}} - \frac{\Sigma n_i-\Sigma y_i}{1-\hat{\theta}}\\
\frac{\Sigma n_i-\Sigma y_i}{1-\hat{\theta}} &=& \frac{\Sigma y_i}{\hat{\theta}} \\
\hat{\theta}(\Sigma n_i-\Sigma y_i) &=& (1-\hat{\theta})\Sigma y_i \\
\hat{\theta}\Sigma n_i - \hat{\theta}\Sigma y_i &=& \Sigma y_i -\hat{\theta}\Sigma y_i \\
\hat{\theta}\Sigma n_i &=& \Sigma y_i \\
\hat{\theta} &=& \frac{\Sigma y_i}{\Sigma n_i} \\
\end{eqnarray*}

\noindent The MLE is calculated to be
\[\hat{\theta}=\frac{73}{476}=0.153.\]

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/1postplot.pdf}
\caption{Plots of the prior (dashed), posterior (solid), and MLE (red).}
\end{center}
\end{figure}


\subsection{Posterior Estimates}

\begin{figure}[H]
\begin{center}
\begin{tabular}{l|r}
Estimate & \multicolumn{1}{l}{Value} \\ \hline \hline
Mean               & $0.1482$ \\
Median             & $0.1478$ \\
Mode               & $0.1469$ \\
Variance           & $0.0002$ \\
Standard Deviation & $0.0154$ \\
\end{tabular}
\end{center}
\end{figure}

\noindent The 95\% central credible interval is $(0.119, 0.179)$.

\subsection{Other Potential Priors}


\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/1otherpriors.pdf}
\caption{Four Posterior Distributions}
\end{center}
\end{figure}

\noindent \emph{Black}.  The priors used in the analysis $a=5$ and $b=45$.

\noindent \emph{Red}. $a=1$, $b=14$.  Based off all home runs and at-bats in seasons previous to 2001.  Very flexible to new data.

\noindent \emph{Blue}. $a=4$, $b=45$.  Looks only at the time Bonds was with the Giants before 2001.  Moderately flexible to new data.

\noindent \emph{Green}. $a=10$, $b=90$.  Uses only the 2000 season to select prior parameters.  New data affects posterior to a lesser degree than other priors.

%\subsection{Conclusion}
%
%In terms of home runs and at-bats, Barry Bonds did exceptionally well in 2001.  He averaged close to 15 home runs for every 100 at-bats.  The lower end of the 95\% credible interval suggests that with high probability Bonds played well enough to hit at least 11\% home runs.  There is also a good chance that he played well enough to hit up to 18\% home runs.  Considering only a handful of players have averaged more than 7\% home runs during their careers, Barry Bonds' 2001 season was indeed spectacular and significant.

\newpage

\section{Failures of the Fastest Computer in the World}

\subsection{Summary}

\noindent We will look at data for 15 months of failures of the Los Alamos National Laboratory ``fasted computer in the world.''  Experts say that the computer should not fail any more than 10 times in a month.

\subsection{The Likelihood and Conjugate Prior}

The probability density function comes from the Poisson distribution, where $y_i$ is the number of failures in a month.  The function, for $i=1,2,\cdots,15$, is:

\[f_*(y_i|\lambda)=\frac{\lambda^{y_i}e^{-\lambda}}{y_i!};\ \ \ \ \ \lambda\geq 0;\ y_i=0,1,\cdots \]

\noindent The likelihood function is:

\[L(\mathbf{y}|\lambda) = \prod_{i=1}^n\left(\frac{\lambda^{y_i}e^{-\lambda}}{y_i!}\right) = \frac{\lambda^{\Sigma y_i}e^{-n\lambda}}{\prod (y_i!)}\]

%\noindent The conjugate prior, as we shall see, is the Gamma distribution, which is:

%\[f(\lambda)=\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda}\]

\noindent Using the gamma distribution (with the rate parametrization)

\[\pi(\lambda)=\frac{b^a}{\Gamma(a)}\lambda^{a-1}e^{-b\lambda};\ \ \ \ \ \lambda\geq 0;\ a,b>0\]

\noindent for our prior, we derive the posterior distribution

\begin{eqnarray*}
\pi(\lambda|\mathbf{y}) &=& \frac{\frac{\lambda^{\Sigma y_i}e^{-n\lambda}}{\prod (y_i!)}\frac{b^a}{\Gamma(a)}\lambda^{a-1}e^{-b\lambda}}{\int_0^\infty\frac{\lambda^{\Sigma y_i}e^{-n\lambda}}{\prod (y_i!)}\frac{b^a}{\Gamma(a)}\lambda^{a-1}e^{-b\lambda}d\lambda} \\
%&=& \frac{\lambda^{\Sigma y_i}e^{-n\lambda}\lambda^{\alpha-1}e^{-\beta\lambda}}{\int_0^\infty\lambda^{\Sigma y_i}e^{-n\lambda}\lambda^{\alpha-1}e^{-\beta\lambda}d\lambda} \\
&=& \frac{\lambda^{a+\Sigma y_i-1}e^{-\lambda(b+n)}}{\int_0^\infty\lambda^{a+\Sigma y_i-1}e^{-\lambda(b+n)}d\lambda} \\
&=& \frac{\lambda^{a+\Sigma y_i-1}e^{-\lambda(b+n)}}{\frac{\Gamma(a+\Sigma y_i)}{(b+n)^{a+\Sigma y_i}}\int_0^\infty\frac{(b+n)^{a+\Sigma y_i}}{\Gamma(a+\Sigma y_i)}\lambda^{a+\Sigma y_i-1}e^{-\lambda(b+n)}d\lambda} \\
&=& \frac{\lambda^{a+\Sigma y_i-1}e^{-\lambda(b+n)}}{\frac{\Gamma(a+\Sigma y_i)}{(b+n)^{a+\Sigma y_i}}} \\
&=& \frac{(b+n)^{a+\Sigma y_i}}{\Gamma(a+\Sigma y_i)}\lambda^{a+\Sigma y_i-1}e^{-\lambda(b+n)} \\
\end{eqnarray*}

\noindent which is a gamma distribution with parameters $a+\Sigma y_i$ and $b+n$.  %Thus we have a conjugate prior.

\subsection{Prior Parameters}

\noindent For this analysis, we chose priors of $a=18$ and $b=3$.  We consider the expert's statement that they thought there ``ought'' to be no more than 10 failures in one month.  There appears to be some uncertainty, even lack of confidence, from the expert.  For the most part, the expert's statement was believed and used as the basis in selecting the priors.
\bigskip

\noindent Such a choice would result in a mean and variance of $\frac{18}{3}=6$ in estimating $\lambda$.  The priors suggest that there is some probability ($0.39$) that the computer will fail more than 10 times in the month, but for the most part should agree with the expert's opinion.

\subsection{Maximum Likelihood and Plots}

%\noindent The plot for a Gamma distribution with parameters $\alpha=18$ and $\beta=3$.
%
%\begin{figure}[H]
%\begin{center}
%\includegraphics[scale=0.5]{figs/2priorplot.pdf}
%\caption{Prior Distribution}
%\end{center}
%\end{figure}

\noindent \emph{Parameters of the posterior distribution.}  From the data collected, there were a total of 34 failures in the 15 months.  So this means we have for posterior parameters $a^*=18+34=52$ and $b^*=3+15=18$.
\bigskip

\noindent \emph{Derivation of the maximum likelihood estimator.}  Take the logarithm of the likelihood function.

\begin{eqnarray*}
\mathrm{log}L(\mathbf{y}|\lambda) &=& \mathrm{log}\left[\frac{\lambda^{\Sigma y_i}e^{-n\lambda}}{\prod (y_i!)}\right] \\
&=& \Sigma y_i\mathrm{log}\lambda-n\lambda-\mathrm{log}\prod (y_i!) \\
\end{eqnarray*}

\noindent Find the derivative of this with respect to $\lambda$.

\begin{eqnarray*}
\frac{d}{d\lambda}\left[\mathrm{log}L(\mathbf{y}|\lambda)\right] &=& \frac{d}{d\lambda}\left[\Sigma y_i\mathrm{log}\lambda-n\lambda-\mathrm{log}\prod (y_i!)\right] \\
&=& \frac{\Sigma y_i}{\lambda}-n+0 \\
&=& \frac{\Sigma y_i}{\lambda}-n \\
\end{eqnarray*}

\noindent Set equal to 0 and solve for $\hat{\lambda}$.

\begin{eqnarray*}
0 &=& \frac{\Sigma y_i}{\hat{\lambda}}-n \\
%n &=& \frac{\Sigma y_i}{\hat{\lambda}} \\
\hat{\lambda} &=& \frac{\Sigma y_i}{n} \\
&=& \ybar \\
\end{eqnarray*}

\noindent which is evaluted to be $\hat{\lambda}=2.26$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/2postplot.pdf}
\caption{Plots of the prior (dashed), posterior (solid), and MLE (red).}
\end{center}
\end{figure}

\subsection{Posterior Estimates}

\begin{figure}[H]
\begin{center}
\begin{tabular}{l|r}
Estimate & \multicolumn{1}{l}{Value} \\ \hline \hline
Mean               & $2.88$ \\
Median             & $2.87$ \\
Mode               & $2.83$ \\
Variance           & $0.16$ \\
Standard Deviation & $0.40$ \\
\end{tabular}
\end{center}
\end{figure}

\noindent The 95\% central credible interval is $(2.15, 3.72)$.

\subsection{Other Potential Priors}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/2otherpriors.pdf}
\caption{Four Posterior Distributions}
\end{center}
\end{figure}

\noindent \emph{Black}.  The priors used in the analysis $a=18$ and $b=3$.

\noindent \emph{Red}. $a=8$, $b=1$.  Not confident in experts response that the computer ``ought" not to fail more than 10 times a month.  Believes a value of 10 or higher is somewhat likely.

\noindent \emph{Blue}. $a=8$, $b=2$.  Most flexible to the data, but leans towards higher probability in lower amounts of failures.

\noindent \emph{Green}. $a=16$, $b=4$.  Has strong confidence in the experts.


%\subsection{Conclusion}
%
%\noindent From our interval estimation we may say that there is a 95\% probability that $\lambda$ is not any less than 2.12 and no higher than 3.68.  Even at the higher end of the interval, having such a $\lambda$ would result in a minimal amount of months where the supercomputer failed 10 or more times.  Though there is still some chance of the computer failing that many times.
%\bigskip
%
%\noindent However, despite the possible occurrence of a high number of failures, using the estimates gathered from the posterior distribution we would realistically expect between 1 and 7 failures in any given month.  Collecting more data will allow us to construct narrower intervals and estimate $\lambda$ with greater accuracy.

\newpage

\section{North American Rivers}

\subsection{Summary}

\noindent We analyze the lengths (in miles) of 141 major rivers in North America.  It is common knowledge that the Mississippi river is one of the world's largest rivers, but this of course isn't typical of most of the major rivers in North America.  So how do river lengths on this continent compare to those around the world?

\subsection{The Likelihood and Conjugate Prior}

\noindent We assume the data is exponentially distributed.  The probability density function is:

\[f_*(y_i|\lambda)=\frac{1}{\lambda}e^{-y_i/\lambda};\ \ \ \lambda>0;\ y_i\geq 0 \]

\noindent The Likelihood function is:

\begin{eqnarray*}
L(\mathbf{y}|\lambda) &=& \prod\left[\frac{1}{\lambda}e^{-y_i/\lambda}\right] \\
&=& \left(\frac{1}{\lambda}\right)^ne^{-\Sigma y_i/\lambda} \\
\end{eqnarray*}

\noindent We will use an inverse gamma distribution for the prior, which has pdf:

\[\pi(\lambda)=\frac{b^a}{\Gamma(a)}\left(\frac{1}{\lambda}\right)^{a+1}e^{-b/\lambda};\ \ \ \lambda>0;\ a>2;\ b>0\]

\noindent Having $a>2$ ensures we have at least the first two moments. The posterior is then

\begin{eqnarray*}
\pi(\lambda|\mathbf{y}) &=& \frac{\left(\frac{1}{\lambda}\right)^ne^{-\Sigma y_i/\lambda}\frac{b^a}{\Gamma(a)}\left(\frac{1}{\lambda}\right)^{a+1}e^{-b/\lambda}}{\int_o^\infty \left(\frac{1}{\lambda}\right)^ne^{-\Sigma y_i/\lambda}\frac{b^a}{\Gamma(a)}\left(\frac{1}{\lambda}\right)^{a+1}e^{-b/\lambda}d\lambda} \\
&=& \frac{\left(\frac{1}{\lambda}\right)^{(a+n)+1}e^{-(b+\Sigma y_i)/\lambda}}{\int_0^\infty \left(\frac{1}{\lambda}\right)^{(a+n)+1}e^{-(b+\Sigma y_i)/\lambda}d\lambda} \\
&=& \frac{\left(\frac{1}{\lambda}\right)^{(a+n)+1}e^{-(b+\Sigma y_i)/\lambda}}{\frac{\Gamma(a+n)}{(b+\Sigma y_i)^{a+n}}\int_0^\infty \frac{(b+\Sigma y_i)^{a+n}}{\Gamma(a+n)}\left(\frac{1}{\lambda}\right)^{(a+n)+1}e^{-(b+\Sigma y_i)/\lambda}d\lambda} \\
&=& \frac{\left(\frac{1}{\lambda}\right)^{(a+n)+1}e^{-(b+\Sigma y_i)/\lambda}}{\frac{\Gamma(a+n)}{(b+\Sigma y_i)^{a+n}}} \\
&=& \frac{(b+\Sigma y_i)^{a+n}}{\Gamma(a+n)}\left(\frac{1}{\lambda}\right)^{(a+n)+1}e^{-(b+\Sigma y_i)/\lambda} \\
\end{eqnarray*}

\noindent which is an inverse gamma distribution with parameters $a^*=a+n$ and $b^*=b+\Sigma y_i$.

%\begin{scriptsize}
%\noindent \emph{Note:} In this analysis we are estimating $\lambda$.  The p.d.f., though a function $\lambda$, is in terms of $1/\lambda$.  This will turn out to be very convenient for us.  The p.d.f is an exponential distribution which has a mean and standard deviation of $1/\lambda$.  Thus with this particular function the mean and standard deviation will simply be $\lambda$.  So when we look at estimates of $\lambda$, we can easily make the connection between the parameter and what we are really interested in, i.e. river lengths in North America.
%\end{scriptsize}

\subsection{Prior Parameters}

\noindent For our analysis we selected priors of $a=5$ and $b=1600$.  With an inverse gamma distribution, this means we have a mean of $\frac{b}{a-1}=400$ and standard deviation of $\sqrt{\frac{b^2}{(a-1)^2(a-2)}}=230.94$.
\bigskip

\noindent We are saying that the average length is about $400$ miles.  With a high standard deviation we have a higher chance of capturing the true mean in our analysis.  Since these priors are comparatively low, the posterior distribution will be more flexible to new data, which is what we want since we have little knowledge of anything about rivers.

\subsection{Maximum Likelihood and Plots}

%The prior distribution with parameters $\alpha=5$ and $\beta=1600$.
%
%\begin{figure}[H]
%\begin{center}
%\includegraphics[scale=0.5]{figs/3priorplot.pdf}
%\caption{Prior Distribution}
%\end{center}
%\end{figure}

\noindent \emph{Parameters of the posterior distribution} With $n=141$ rivers and $\Sigma y_i=83357$, for the posterior distribution we have parameters $a^*=5+141=146$ and $b^*=1600+83357=84957$.
\bigskip

\noindent \emph{Derivation of the maximum likelihood estimator.} Begin, as before, by taking the logarithm of the likelihood function and finding the derivative with respect to $\lambda$.

\begin{eqnarray*}
\mathrm{log}L(\mathbf{y}|\lambda) &=& \mathrm{log}\left[\left(\frac{1}{\lambda}\right)^ne^{-\Sigma y_i/\lambda}\right] \\
&=& -n\mathrm{log}\lambda-\frac{\Sigma y_i}{\lambda} \\
\frac{d}{d\lambda}\mathrm{log}L(\mathbf{y}|\lambda) &=& \frac{d}{d\lambda}\left[-n\mathrm{log}\lambda-\frac{\Sigma y_i}{\lambda}\right] \\
&=& -\frac{n}{\lambda}+\frac{\Sigma y_i}{\lambda^2} \\
\end{eqnarray*}

\noindent Set equal to 0 and solve for $\hat{\lambda}$.

\begin{eqnarray*}
0 &=& -\frac{n}{\hat{\lambda}}+\frac{\Sigma y_i}{\hat{\lambda}^2} \\
\frac{n}{\hat{\lambda}} &=& \frac{\Sigma y_i}{\hat{\lambda}^2} \\
%n\hat{\lambda} &=& \Sigma y_i \\
\hat{\lambda} &=& \frac{\Sigma y_i}{n} \\
&=& \ybar \\
\end{eqnarray*}

\noindent We calculate $\hat{\lambda}=591.1$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/3postplot.pdf}
\caption{Plots of the prior (dashed), posterior (solid), and MLE (red).}
\end{center}
\end{figure}

\subsection{Posterior Estimates}

%\noindent To compute the median and credible interval, we took a large random sample of the inverse gamma and calculated the appropriate quantiles.

\begin{figure}[H]
\begin{center}
\begin{tabular}{l|r}
Estimate & \multicolumn{1}{l}{Value} \\ \hline \hline
Mean               & $585.9$ \\
Median             & $583.2$ \\
Mode               & $577.9$ \\
Variance           & $2838.9$ \\
Standard Deviation & $48.8$ \\
\end{tabular}
\end{center}
\end{figure}

\noindent The 95\% central credible interval is $(497.0, 687.8)$.

\subsection{Other Potential Priors}

%\ \\ [-1.5cm]

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/3otherpriors.pdf}
\caption{Four Posterior Distributions}
\end{center}
\end{figure}

\noindent \emph{Black}.  The priors used in the analysis $a=5$ and $b=1600$.

\noindent \emph{Red}. $a=2$, $b=1000$.  Most flexible to new data. Little knowledge of rivers.

\noindent \emph{Blue}. $a=9$, $b=3000$.  Attempts to agree with the lengths of the rivers in North America based on a brief Internet search.

\noindent \emph{Green}. $a=6$, $b=4000$.  Priors based off a quick look at top river lengths throughout the world. These parameters attempt to roughly match the mean from rivers worldwide.
%\bigskip

%\noindent Due to the large amount of data, it turns out that each of these choices in priors resulted in roughly the same posterior distribution.

%\subsection{Conclusion}
%
%\noindent We found that the posterior distribution for our parameter of interest $\lambda$ resulted in a mean of 585.  Credible interval bounds tells us that $\lambda$ has a 95\% probability of being within 493 and 683.  Now what we are interested in is what this tells us about river lengths in North America.  Conveniently enough, $\lambda$ corresponds with the mean length and standard deviation of rivers.  So the mean and standard deviation is $\lambda$.
%\bigskip
%
%\noindent The mode posterior distribution indicates to us that the most likely that $\lambda$ could be is 577.  That is, in North America, the average river length is 577.  Now these rivers follow an exponential distribution which has a very long tail, so we aren't surprised when rivers such as the Mississippi or Missouri show up in our data.  But most our river lengths will be around 600 miles or shorter.
%\bigskip
%
%\noindent Since this data came from North America's major rivers we would like to see how this compares to the world's major rivers.  Keep in mind that most rivers in North America will about no more than 600 or 650 miles long.  When considering the world's longest rivers, the river ranked 175th sits at about 645 miles.  So if we were to do an analysis on the world rivers, we would expect $\lambda$ to be somewhere close to 1000.  Not much of a competition for North America.

\newpage

\section{Statistics 221 Honors Students - (1)}

\subsection{Summary}

\noindent We analyze the test scores from the first exam in an honors section of Statistics 221.  This is an introductory course and we are told that the exam is ``extremely easy.''

\subsection{The Likelihood and Conjugate Prior}

\noindent We will assume the data is normally distributed with mean $\mu$ and $\sigma=9$.  The probability density function is

\begin{eqnarray*}
f_*(y_i|\mu,\sigma)=(2\pi\sigma^2)^{-1/2}\exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right); && -\infty<y_i,\mu<+\infty; \\
&& \sigma^2>0 \\
\end{eqnarray*}

\noindent The likelihood function is

\begin{eqnarray*}
L(\mathbf{y}|\mu) &=& \prod\left[(2\pi\sigma^2)^{-1/2}\exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right)\right] \\
&=& (2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2\right) \\
\end{eqnarray*}

\noindent We will use a normal distribution for the prior, assuming $\mu\sim N(m,s^2)$. We consider first the numerator (or the expression inside the integral in the denominator). Since the integral in the denominator is with respect to $\mu$, any terms not containing a $\mu$ will cancel.

\begin{eqnarray*}
\pi(\mu|\mathbf{y}) &\propto& (2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2\right)(2\pi s^2)^{-1/2}\exp\left(-\frac{1}{2s^2}(\mu-m)^2\right) \\
&\propto& \exp\left(-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2\right)\exp\left(-\frac{1}{2 s^2}(\mu-m)^2\right) \\
&\propto& \exp\left(-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2-\frac{1}{2 s^2}(\mu-m)^2\right) \\
&\propto& \exp\left(-\frac{1}{2\sigma^2}(\Sigma y_i^2-2\mu\Sigma y_i+n\mu^2)-\frac{1}{2 s^2}(\mu^2-2\mu m+m^2)\right) \\
&\propto& \exp\left(-\frac{1}{2\sigma^2 s^2}\left(s^2(\Sigma y_i^2-2\mu\Sigma y_i+n\mu^2)+\sigma^2(\mu^2-2\mu m+m^2)\right)\right) \\
&\propto& \exp\left(-\frac{1}{2\sigma^2 s^2}\left(s^2(n\mu^2-2\mu\Sigma y_i)+\sigma^2(\mu^2-2\mu\mu_0)\right)\right) \\
&\propto& \exp\left(-\frac{1}{2\sigma^2 s^2}\left(s^2n\mu^2-s^22\mu\Sigma y_i+\sigma^2\mu^2-\sigma^22\mu m\right)\right) \\
&\propto& \exp\left(-\frac{1}{2\sigma^2 s^2}\left(\mu^2(s^2n+\sigma^2)-2\mu(s^2\Sigma y_i-\sigma^2m)\right)\right) \\
&\propto& \exp\left[-\frac{1}{2\sigma^2 s^2}\left((s^2n+\sigma^2)\left[\mu^2-2\mu\left(\frac{s^2\Sigma y_i-\sigma^2m}{s^2n+\sigma^2}\right)\right]\right)\right] \\
&\propto& \exp\left[-\frac{s^2n+\sigma^2}{2\sigma^2s^2}\left(\mu^2-2\mu\left(\frac{s^2n\ybar-\sigma^2m}{s^2n+\sigma^2}\right)\right)\right] \\
\end{eqnarray*}

\noindent For simplicity in the remainder of the derivation we will let 

\[m^*=\frac{s^2n\ybar-\sigma^2m}{s^2n+\sigma^2}\ \ \mathrm{and}\ \ s^{*2}=\left(\frac{s^2n+\sigma^2}{\sigma^2s^2}\right)^{-1}\]
\noindent then proceed

\begin{eqnarray*}
\pi(\mu|\mathbf{y}) &\propto& \exp\left[-\frac{1}{2s^{*2}}\left(\mu^2-2\mu m^*\right)\right] \\
&\propto& \exp\left[-\frac{1}{2s^{*2}}\left(\mu^2-2\mu m^*+m^{*2}-m^{*2}\right)\right] \\
&\propto& \exp\left[-\frac{1}{2s^{*2}}\left(\mu^2-2\mu m^*+m^{*2}\right)\right] \\
&\propto& \exp\left[-\frac{1}{2s^{*2}}\left(\mu-m^*\right)^2\right] \\
\end{eqnarray*}

\noindent To clarify, all of the operations done on the numerator are also performed on the denominator. The constants terms (with respect to $\mu$) are canceled. Returning to the posterior in its full form we have

\begin{eqnarray*}
\pi(\mu|\mathbf{y}) &=& \frac{\exp(-\frac{1}{2s^{*2}}(\mu-m^*)^2)}{\int_{-\infty}^\infty \exp(-\frac{1}{2s^{*2}}(\mu-m^*)^2)d\mu} \\
&=& \frac{\exp(-\frac{1}{2s^{*2}}(\mu-m^*)^2)}{(2\pi s^{*2})^{1/2}\int_{-\infty}^\infty(2\pi s^{*2})^{-1/2}\exp(-\frac{1}{2s^{*2}}(\mu-m^*)^2)d\mu} \\
&=& \frac{\exp(-\frac{1}{2s^{*2}}(\mu-m^*)^2)}{(2\pi s^{*2})^{1/2}} \\
&=& (2\pi s^{*2})^{-1/2}\exp\left(-\frac{1}{2s^{*2}}(\mu-m^*)^2\right) \\
\end{eqnarray*}

\noindent This is a normal distribution with mean $m^*$ and variance $s^{*2}$, as given above.

\subsection{Prior Parameters}

\noindent Since $\sigma^2$ is given and $\mu\sim N(m,s^2)$ we only need to choose prior values for $m$ and $s^2$.  We are told that the exam is extremely easy.  Since 70\% is typically thought of to be the average grade on a test, we will interpret ``extremely easy" to mean that the average is about 85\%, with some room for error.  So we will go with priors of $m=85$ and $s^2=4$.
\bigskip

\subsection{Maximum Likelihood and Plots}

\noindent \emph{Parameters of the posterior distribution.} $m^*=86.34$ and $s^{*2}=1.64$.
\bigskip

\noindent \emph{Derivation of the maximum likelihood estimator.} Proceed!

\begin{eqnarray*}
\mathrm{log}L(\mathbf{y}|\mu) &=& \mathrm{log}\left[(2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2\right)\right] \\
&=& -\frac{n}{2}\mathrm{log}(2\pi\sigma^2)-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2 \\
\frac{d}{d\mu}\mathrm{log}L(\mathbf{y}|\mu) &=& \frac{d}{d\mu}\left[-\frac{n}{2}\mathrm{log}(2\pi\sigma^2)-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2\right] \\
&=& -\frac{1}{2\sigma^2}\Sigma(y_i-\mu)(2)(-1) \\
&=& \frac{1}{\sigma^2}(\Sigma y_i-n\mu) \\
\end{eqnarray*}

\noindent Set equal to 0 and solve for $\hat{\mu}$

\begin{eqnarray*}
0 &=& \frac{1}{\sigma^2}(\Sigma y_i-n\hat{\mu}) \\
0 &=& (\Sigma y_i-n\hat{\mu}) \\
n\hat{\mu} &=& \Sigma y_i \\
%\hat{\mu} &=& \frac{\Sigma y_i}{n} \\
&=& \ybar \\
\end{eqnarray*}

\noindent with $\hat{\mu}=87.27$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/4postplot.pdf}
\caption{Plots of the prior (dashed), posterior (solid), and MLE (red).}
\end{center}
\end{figure}

\subsection{Posterior Estimates}

\begin{figure}[H]
\begin{center}
\begin{tabular}{l|r}
Estimate & \multicolumn{1}{l}{Value} \\ \hline \hline
Mean               & $86.34$ \\
Median             & $86.34$ \\
Mode               & $86.34$ \\
Variance           & $1.64$ \\
Standard Deviation & $1.28$ \\
\end{tabular}
\end{center}
\end{figure}

\noindent The 95\% central credible interval is $(83.11, 89.56)$.


\subsection{Other Potential Priors}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/4otherpriors.pdf}
\caption{Four Posterior Distributions}
\end{center}
\end{figure}

\noindent \emph{Black}.  The priors used in the analysis $m=85$ and $s^2=4$.

\noindent \emph{Red}. $m=90$, $s^2=1$.  The justification of these priors comes from another interpretation of ``extremely easy" suggesting that the average test score is quite high.

\noindent \emph{Blue}. $m=75$, $s^2=25$.  Higher variability in what the mean could be, but considers the average score to center around 75\%.

\noindent \emph{Green}. $m=80$, $s^2=16$.  Similar to that used in the analysis, but with more variability.  This is to take into account a range of higher scores.

%\subsection{Conclusion}
%
%The analysis showed that the there is a 95\% probability that the average on the test score is between 83.1 and 89.5.  We normally think that an average grade on an exam should be about 70\%.  We may be tempted to agree with the test's description of extremely easy, but we should not forget that an honors section took the test.  Therefore it will be hard for us to conclude that the exam in reality is easy.  However, we are safe in saying, with good certainty as per the credible interval, that the mean is about 86.3\%.

\newpage

\section{Statistics 221 Honors Students - (2)}

\subsection{Summary}

\noindent We perform the same analysis as before, but this time we assume the data is distributed normally with parameters $\mu=87$ and unknown $\sigma$.  Therefore, we will be interested in estimating the variance.

\subsection{The Likelihood and Conjugate Prior}

\noindent The probability density function is

\begin{eqnarray*}
f_*(y_i|\mu,\sigma)=(2\pi\sigma^2)^{-1/2}\exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right); && -\infty<y_i,\mu<+\infty; \\
&& \sigma^2>0 \\
\end{eqnarray*}

\noindent The likelihood function is

\begin{eqnarray*}
L(\mathbf{y}|\sigma) &=& \prod\left[(2\pi\sigma^2)^{-1/2}\exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right)\right] \\
&=& (2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\Sigma(y_i-\mu)^2\right) \\
\end{eqnarray*}

\noindent We will use the inverse gamma distribution for the prior.  Let $\theta=\sigma^2$.  The posterior distribution is then

\begin{eqnarray*}
\pi(\theta|\mathbf{y}) &\propto& (2\pi\theta)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\theta}\Sigma(y_i-\mu)^2\right)\frac{b^a}{\Gamma(a)}\theta^{-a-1}\exp(-b/\theta) \\
&\propto& \theta^{-\frac{n}{2}}\exp\left(-\frac{1}{2\theta}\Sigma(y_i-\mu)^2\right)\theta^{-a-1}\exp(-b/\theta) \\
&\propto& \theta^{-(a+\frac{n}{2})-1}\exp\left(-\frac{1}{\theta}\left(\frac{\Sigma(y_i-\mu)^2}{2}+b\right)\right) \\
\end{eqnarray*}

\noindent Let $a^*=a+\frac{n}{2}$ and $b^*=\left(\frac{\Sigma(y_i-\mu)^2}{2}+b\right)$ then

\begin{eqnarray*}
\pi(\theta|\mathbf{y}) &=& \frac{\theta^{-a^*-1}\exp(-b^*/\theta)}{\int_0^\infty\theta^{-a^*-1}\exp(-b^*/\theta)d\theta} \\
&=& \frac{\theta^{-a^*-1}\exp(-b^*/\theta)}{\frac{\Gamma(a^*)}{(b^*)^{a^*}}\int_0^\infty\frac{(b^*)^{a^*}}{\Gamma(a^*)}\theta^{-a^*-1}\exp(-b^*/\theta)d\theta} \\
%&=& \frac{\theta^{-a^*-1}\exp(-b^*/\theta)}{\frac{\Gamma(a^*)}{(b^*)^{a^*}}} \\
&=& \frac{(b^*)^{a^*}}{\Gamma(a^*)}\theta^{-a^*-1}\exp(-b^*/\theta) \\
\end{eqnarray*}

\noindent This is an inverse gamma with parameters $a^*=a+\frac{n}{2}$ and $b^*=\left(\frac{\Sigma(y_i-\mu)^2}{2}+b\right)$

\subsection{Prior Parameters}

\noindent Since we are given that $\mu=87$, we know we don't want a $\sigma^2$ that is too high since the max score is 100 and we would prefer not to go beyond that.  It seems appropriate to let a prior on $\sigma^2$ that causes the standard deviation to be about 6 or 7.  This means that two standard deviations above the mean will catch the max score and still go low enough to get the lower scores.  And since we are assuming the exam is easy, we wouldn't expect to go too far below the mean.

\subsection{Maximum Likelihood and Plots}

\noindent \emph{Parameters of the posterior.} $a^*=16.5$ and $b^*=1136$.
\bigskip

\noindent \emph{Derivation of the maximum likelihood estimator.}  Let $\theta=\sigma^2$ to avoid confusion in the derivative.  The log likelihood function is

\begin{eqnarray*}
\mathrm{log}L(\mathbf{y}|\theta) &=& \mathrm{log}\left[(2\pi\theta)^{-n/2}\exp\left(-\frac{1}{2\theta}\Sigma(y_i-\mu)^2\right)\right] \\
&=& -\frac{n}{2}\mathrm{log}(2\pi\theta)-\frac{1}{2\theta}\Sigma(y_i-\mu)^2 \\
\frac{d}{d\theta}\mathrm{log}L(\mathbf{y}|\theta) &=& \frac{d}{d\theta}\left[-\frac{n}{2}\mathrm{log}(2\pi\theta)-\frac{1}{2\theta}\Sigma(y_i-\mu)^2\right] \\
&=& -\frac{n}{2\theta}-\frac{1}{2\theta^2}\Sigma(y_i-\mu)^2(-1) \\
&=& -\frac{n}{2\theta}+\frac{1}{2\theta^2}\Sigma(y_i-\mu)^2 \\
\end{eqnarray*}

\noindent Set equal to 0 and solve for $\hat{\theta}=\hat{\sigma}^2$

\begin{eqnarray*}
0 &=& -\frac{n}{2\hat{\theta}}+\frac{1}{2\hat{\theta}^2}\Sigma(y_i-\mu)^2 \\
0 &=& -n\hat{\theta}+\Sigma(y_i-\mu)^2 \\
n\hat{\theta} &=& \Sigma(y_i-\mu)^2 \\
\hat{\sigma}^2=\hat{\theta} &=& \frac{\Sigma(y_i-\mu)^2}{n} \\
\end{eqnarray*}


\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/5postplot.pdf}
\caption{Plots of the prior (dashed), posterior (solid), and MLE (red).}
\end{center}
\end{figure}

\subsection{Posterior Estimates}

\begin{figure}[H]
\begin{center}
\begin{tabular}{l|r}
Estimate & \multicolumn{1}{l}{Value} \\ \hline \hline
Mean               & $73.2$ \\
Median             & $70.1$ \\
Mode               & $64.9$ \\
Variance           & $370.4$ \\
Standard Deviation & $19.2$ \\
\end{tabular}
\end{center}
\end{figure}

The 95\% central credible interval is $(45.1, 119.3)$.

\subsection{Other Potential Priors}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figs/5otherpriors.pdf}
\caption{Four Posterior Distributions}
\end{center}
\end{figure}

\noindent \emph{Black}.  The priors used in the analysis $a=2$ and $b=50$.

\noindent \emph{Red}. $a=2$, $b=150$.  Variability is rather uncertain, so let it spread out further and be more flexible to new data.

\noindent \emph{Blue}. $a=4$, $b=60$.  Presumes to be rather confident in choice of $\sigma^2$.  Priors have more weight, less flexible to new data.

\noindent \emph{Green}. $a=5$, $b=120$.  Considers variability to be in a higher range, thinking that ``extremely easy" doesn't apply to all who took the test.

%\subsection{Conclusion}
%
%Taking the square root of our estimates will be most helpful in analyzing the distribution of the data.  For an HPD interval, we get $(6.44, 10.56)$ and this gives us a good idea of how our data are distributed.  In comparison to what we might normally expect with exam grades (say about 10), these standard deviations are small.  This suggests that, as a whole, the class did well and scored pretty close together, without too many low scores.

\end{document}
