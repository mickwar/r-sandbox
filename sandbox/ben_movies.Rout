
R version 3.1.0 (2014-04-10) -- "Spring Dance"
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(AUC)
AUC 0.3.0
Type AUCNews() to see the change log and ?AUC to get an overview.
> library(splines)
> 
> # data cleaning
> dat = read.table("../data/movie.txt", header=T)
> dat = dat[,-2]
> 
> dat = cbind(dat, "kids_I"=is.na(dat$kids_S)*1)
> for (i in 2:4)
+     dat[,i] = ifelse(is.na(dat[,i]), 0, dat[,i])
> dat = cbind(dat, "rot_I"=is.na(dat$rot_top)*1)
> for (i in 6:8)
+     dat[,i] = ifelse(is.na(dat[,i]), 0, dat[,i])
> 
> # put the rotten tomatoes in percentages (easier to
> # interpret)
> dat[,6:8] = 100*dat[,6:8]
> 
> # functions
> binary.matrix = function(x){
+     level = unique(x)
+     n = length(x)
+     k = length(level)
+     out = matrix(0, n, k)
+     for (i in 1:k)
+         out[which(x == level[i]), i] = 1
+     return (out)
+     }
> mw.smooth = function(x, y, d=1){
+     kern = function(x, y, d)
+         exp(-1/(2*d^2)*(x-y)^2)
+     m = 8*length(x)
+     outx = seq(min(x), max(x), length=m)
+     outy = double(m)
+     for (i in 1:length(outy))
+         outy[i] = sum(y*kern(x, outx[i], d))/
+             sum(kern(x, outx[i], d))
+     return (list("x"=outx, "y"=outy))
+     }
> 
> # assumption checking
> # check for non-monotonicity
> # possible issues in kids_P, rot_all
> n = nrow(dat)
> 
> CC = dat$kids_V
> SS = sort(unique(CC))
> means = double(length(SS))
> for (i in 1:length(means))
+     means[i] = mean(dat[which(CC == SS[i]), 1])
> smooth = mw.smooth(SS, means, 0.5)
> plot(CC+rnorm(n,0,0.15),dat$ben+rnorm(n,0,0.015),pch=20,cex=0.5)
> lines(smooth$x, smooth$y, col='red')
> 
> CC = dat$rot_top
> SS = sort(unique(CC))
> means = double(length(SS))
> for (i in 1:length(means))
+     means[i] = mean(dat[which(CC == SS[i]), 1])
> smooth = mw.smooth(SS, means, 5)
> plot(CC+rnorm(n,0,0.15),dat$ben+rnorm(n,0,0.015),pch=20,cex=0.5)
> lines(smooth$x, smooth$y, col='red')
> 
> CC = dat$rot_all
> SS = sort(unique(CC))
> means = double(length(SS))
> for (i in 1:length(means))
+     means[i] = mean(dat[which(CC == SS[i]), 1])
> smooth = mw.smooth(SS, means, 5)
> plot(CC+rnorm(n,0,0.15),dat$ben+rnorm(n,0,0.015),pch=20,cex=0.5)
> lines(smooth$x, smooth$y, col='red')
> 
> CC = dat$rot_aud
> SS = sort(unique(CC))
> means = double(length(SS))
> for (i in 1:length(means))
+     means[i] = mean(dat[which(CC == SS[i]), 1])
> smooth = mw.smooth(SS, means, 5)
> plot(CC+rnorm(n,0,0.15),dat$ben+rnorm(n,0,0.015),pch=20,cex=0.5)
> lines(smooth$x, smooth$y, col='red')
> # Y = as.vector(dat[,1])
> # X = as.matrix(cbind(dat[,c(2:4, 6)], binary.matrix(dat[,5])))
> # colnames(X)[5:8] = c("PG13", "PG", "G", "NR")
> # mod = glm(Y ~ X, family=binomial)
> # summary(mod)
> 
> # removing the intercept makes things difficult to interpret,
> # so don't do this
> # mod2 = glm(ben ~ -1 + . , family=binomial, data=dat)
> # summary(mod2)
> 
> full.mod = glm(ben ~ ns(kids_P, df=2) + ns(rot_all, df=2) +
+     kids_S + kids_V + rot_top + rot_aud + MPAA + kids_I + rot_I,
+     data=dat, family=binomial)
> null.mod = glm(ben ~ 1, data=dat, family=binomial)
> 
> mod = step(null.mod, scope=list(lower=null.mod, upper=full.mod),
+     k=log(nrow(dat)), data=dat, direction="both", family=binomial)
Start:  AIC=111.37
ben ~ 1

                      Df Deviance     AIC
+ kids_S               1   87.243  96.105
+ rot_top              1   96.178 105.039
+ rot_aud              1   96.980 105.841
+ ns(rot_all, df = 2)  2   94.489 107.781
+ kids_V               1   99.460 108.322
<none>                    106.934 111.365
+ ns(kids_P, df = 2)   2  101.024 114.316
+ kids_I               1  105.794 114.656
+ rot_I                1  106.438 115.300
+ MPAA                 3   99.238 116.961

Step:  AIC=96.1
ben ~ kids_S

                      Df Deviance     AIC
+ rot_top              1   67.237  80.530
+ ns(rot_all, df = 2)  2   69.500  87.223
+ rot_aud              1   76.881  90.173
+ rot_I                1   82.396  95.688
<none>                     87.243  96.105
+ kids_I               1   84.662  97.954
+ kids_V               1   87.239 100.532
+ ns(kids_P, df = 2)   2   86.641 104.364
+ MPAA                 3   84.660 106.814
- kids_S               1  106.934 111.365

Step:  AIC=80.53
ben ~ kids_S + rot_top

                      Df Deviance     AIC
<none>                     67.237  80.530
+ rot_aud              1   65.530  83.253
+ kids_V               1   66.508  84.231
+ rot_I                1   67.112  84.835
+ kids_I               1   67.176  84.899
+ ns(rot_all, df = 2)  2   66.727  88.881
+ ns(kids_P, df = 2)   2   66.775  88.929
+ MPAA                 3   65.706  92.291
- rot_top              1   87.243  96.105
- kids_S               1   96.178 105.039
> summary(mod)

Call:
glm(formula = ben ~ kids_S + rot_top, family = binomial, data = dat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8976  -0.6870   0.2727   0.5433   2.0132  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.25002    0.71204  -0.351 0.725489    
kids_S      -1.59280    0.38453  -4.142 3.44e-05 ***
rot_top      0.05168    0.01341   3.854 0.000116 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 106.934  on 83  degrees of freedom
Residual deviance:  67.237  on 81  degrees of freedom
AIC: 73.237

Number of Fisher Scoring iterations: 5

> 
> n = nrow(dat)
> train.n = ceiling(n * 1)
> m = 100000
> k = 1000
> l = 50
> pos.rate = matrix(0, m, k)
> neg.rate = matrix(0, m, k)
> error = matrix(0, m, k)
> all.roc = matrix(0, m, l)
> 
> auc.total = double(m)
> for (i in 1:m){
+     # strict subset
+ #   training.obs = sample(1:n,train.n)
+     # bootstrap
+     training.obs = sample(n, train.n, replace=TRUE)
+     test.obs = c(1:n)[-training.obs]
+     training.data = dat[training.obs, ]
+     test.data = dat[test.obs, ]
+ 
+     # model with significant parameters
+     train.mod = glm(ben ~ 1 + kids_S + rot_top, family = binomial,
+         data = dat, subset = training.obs)
+     # model that just fits everything
+ #   train.mod = glm(ben ~ ., family = binomial, data = dat)
+ 
+     risk.score = predict(train.mod, newdata=test.data)
+ 
+     p = exp(risk.score)/(1+exp(risk.score))
+     ROC = roc(p, as.factor(test.data[,1]))
+     auc.total[i] = auc(ROC)
+ #   plot(ROC$fpr, ROC$tpr, ylab="True Positive Rate", type='s', lwd=2,
+ #       main="Receiver Operating Characteristic Curve",
+ #       xlab="False Positive Rate")
+ #   abline(0, 1, lty=2, col="gray")
+     app = approx(ROC$fpr, ROC$tpr, xout = seq(0,1, length=l))
+     all.roc[i,] = app$y
+ #   points(app$x, app$y, ylab="True Positive Rate", type='l', lwd=2,
+ #       main="Receiver Operating Characteristic Curve", col='blue',
+ #       xlab="False Positive Rate")
+ #   abline(0, 1, lty=2, col="gray")
+ 
+ #   lower = min(p)
+ #   upper = max(p)
+     lower = 0
+     upper = 1
+     cutoff = seq(lower,upper,length=k)
+ 
+     for (j in 1:k){
+         t.pos = sum(test.data[ ,1] == 1 & p > cutoff[j])
+         t.neg = sum(test.data[ ,1] == 0 & p < cutoff[j])
+         total.neg = n-length(table(training.obs))-sum(test.data[ ,1])
+         total.pos = sum(test.data[ ,1])
+ 
+         pos.rate[i, j] = 1-t.pos/total.pos
+         neg.rate[i, j] = 1-t.neg/total.neg
+         error[i, j] = 1-(t.pos+t.neg)/(n-length(table(training.obs)))
+         }
+ 
+ #   readline()
+     }
Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> remmy = which(is.na(pos.rate[,1]))
> if (length(remmy) > 0){
+     pos.rate = pos.rate[-remmy,]
+     neg.rate = neg.rate[-remmy,]
+     error = error[-remmy,]
+     }
> 
> plot(cutoff, apply(pos.rate, 2, mean), type='l', col="darkblue", lwd=2,
+     ylab="Error Rate", xlab="Threshold", main="Predictive Error Rates")
> points(cutoff, apply(pos.rate, 2, quantile, 0.975), type='l',
+     col="blue", lty=2, lwd=0.5)
> points(cutoff, apply(pos.rate, 2, quantile, 0.025), type='l',
+     col="blue", lty=2, lwd=0.5)
> 
> points(cutoff, apply(neg.rate, 2, mean), type='l', col="red2", lwd=2)
> points(cutoff, apply(neg.rate, 2, quantile, 0.975), type='l',
+     col="red", lty=2, lwd=0.5)
> points(cutoff, apply(neg.rate, 2, quantile, 0.025), type='l',
+     col="red", lty=2, lwd=0.5)
> 
> points(cutoff, apply(error, 2, mean), type='l', col="darkgreen", lwd=2)
> points(cutoff, apply(error, 2, quantile, 0.975), type='l',
+     col="green", lty=2, lwd=0.5)
> points(cutoff, apply(error, 2, quantile, 0.025), type='l',
+     col="green", lty=2, lwd=0.5)
> 
> legend(0.4, 1, col=c("darkblue", "red2", "darkgreen"), lwd=c(1,1,2),
+     legend=c("False Positive","False Negative","Overall Error"),
+     cex = 1.1, lty = 1)
> 
> plot(seq(0, 1, length=l), apply(all.roc, 2, mean), type='l', lwd=2,
+     ylab="True Positive Rate", col='blue', ylim=c(0, 1),
+     main="Receiver Operating Characteristic Curve",
+     xlab="False Positive Rate")
> points(seq(0, 1, length=l), apply(all.roc, 2, quantile, 0.025),
+     type='l', lty=2, col='lightblue')
> points(seq(0, 1, length=l), apply(all.roc, 2, quantile, 0.975),
+     type='l', lty=2, col='lightblue')
> abline(0, 1, lty=2, col="gray", lwd=2)
> 
> merror = apply(error, 2, mean)
> lerror = apply(error, 2, quantile, 0.025)
> uerror = apply(error, 2, quantile, 0.975)
> at = which.min(merror)
> c(lerror[at], merror[at], uerror[at], cutoff[at])
[1] 0.1000000 0.2145899 0.3437500 0.7257257
> mean(auc.total)
[1] 0.8750872
> 
> 
> # false positive - predicting a "like" when not really liked
> # false negative - predicting a "dislike" when really liked
> 
> 
> 
> proc.time()
     user    system   elapsed 
48982.591   133.589 49120.868 
